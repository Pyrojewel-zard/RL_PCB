#!/bin/bash

# RL_PCB æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”å®éªŒ
# è¯¥è„šæœ¬å¯¹æ¯”åŸå§‹ç‰ˆæœ¬ä¸æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬çš„è®­ç»ƒæ•ˆæœ

echo "ğŸš€ RL_PCB æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”å®éªŒå¯åŠ¨"
echo "=========================================="

# ç³»ç»Ÿèµ„æºæ£€æµ‹
CPU_CORES=$(nproc)
GPU_COUNT=$(nvidia-smi -L 2>/dev/null | wc -l || echo 0)

echo "=== ç³»ç»Ÿèµ„æºæ£€æµ‹ ==="
echo "CPUæ ¸å¿ƒæ•°: $CPU_CORES"
echo "GPUæ•°é‡: $GPU_COUNT"
if [ $GPU_COUNT -gt 0 ]; then
    echo "GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
fi
echo "====================="

# è®¾ç½®ç¯å¢ƒå˜é‡
export OMP_NUM_THREADS=$CPU_CORES
export MKL_NUM_THREADS=$CPU_CORES
export NUMEXPR_NUM_THREADS=$CPU_CORES

# å®éªŒç›®å½•è®¾ç½®
EXP_DIR=${PWD}
export EXP_DIR=${EXP_DIR}
echo "å®éªŒç›®å½•: ${EXP_DIR}"
echo "RL_PCBæ ¹ç›®å½•: ${RL_PCB}"

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p work
mkdir -p performance_logs

# åˆ›å»ºæ€§èƒ½è®°å½•æ–‡ä»¶
PERF_LOG="${EXP_DIR}/performance_logs/performance_comparison.log"
touch $PERF_LOG

echo "$(date): æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”å®éªŒå¼€å§‹" >> $PERF_LOG

# å‡½æ•°ï¼šè®°å½•æ€§èƒ½æ•°æ®
log_performance() {
    local experiment_name=$1
    local start_time=$2
    local end_time=$3
    local gpu_utilization=$4
    
    local duration=$((end_time - start_time))
    echo "$(date): $experiment_name - è€—æ—¶: ${duration}ç§’, GPUåˆ©ç”¨ç‡: ${gpu_utilization}%" >> $PERF_LOG
}

# å‡½æ•°ï¼šå¯åŠ¨TensorBoard
start_tensorboard() {
    local port=${1:-6006}
    
    # å…³é—­å·²æœ‰çš„TensorBoardè¿›ç¨‹
    echo "å…³é—­ç°æœ‰TensorBoardè¿›ç¨‹..."
    pkill -f "tensorboard.*--port.*" || true
    sleep 2
    
    echo "åœ¨ç«¯å£ $port å¯åŠ¨TensorBoard..."
    tensorboard --logdir ./work/ --host 0.0.0.0 --port $port &
    sleep 3
    echo "TensorBoardå·²å¯åŠ¨: http://localhost:$port"
}

# å‡½æ•°ï¼šGPUç›‘æ§
monitor_gpu() {
    local experiment_name=$1
    local duration_minutes=$2
    
    echo "å¼€å§‹ç›‘æ§GPUä½¿ç”¨æƒ…å†µ ($experiment_name)..."
    nvidia-smi dmon -s pucvmet -d 5 -c $((duration_minutes * 12)) > "${EXP_DIR}/performance_logs/gpu_${experiment_name}.log" &
    echo "GPUç›‘æ§å·²å¯åŠ¨ï¼Œæ—¥å¿—: ${EXP_DIR}/performance_logs/gpu_${experiment_name}.log"
}

# å¯åŠ¨TensorBoard
start_tensorboard 6006

echo ""
echo "ğŸ¯ å¼€å§‹æ€§èƒ½å¯¹æ¯”å®éªŒ"
echo "å®éªŒåŒ…æ‹¬:"
echo "1. åŸºçº¿å®éªŒ (åŸå§‹ç‰ˆæœ¬)"
echo "2. ä¼˜åŒ–å®éªŒ (å¤šçº¿ç¨‹+GPUä¼˜åŒ–)"
echo "3. æ¶ˆèå®éªŒ (åˆ†åˆ«æµ‹è¯•å„ç»„ä»¶)"
echo ""

# æ£€æŸ¥ä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒè„šæœ¬æ˜¯å¦å­˜åœ¨
if [ ! -f "${RL_PCB}/src/training/train_optimized.py" ]; then
    echo "âŒ é”™è¯¯: æ‰¾ä¸åˆ°ä¼˜åŒ–ç‰ˆæœ¬è®­ç»ƒè„šæœ¬"
    echo "è¯·ç¡®ä¿ train_optimized.py å·²åˆ›å»ºåœ¨ src/training/ ç›®å½•ä¸‹"
    exit 1
fi

# åˆ‡æ¢åˆ°è®­ç»ƒç›®å½•
cd ${RL_PCB}/src/training

# echo ""
# echo "ğŸ“Š é˜¶æ®µ 1: åŸºçº¿æ€§èƒ½æµ‹è¯• (åŸå§‹ç‰ˆæœ¬)"
# echo "=========================================="

# # SACåŸºçº¿æµ‹è¯•
# echo "ğŸ”¬ è¿è¡ŒSACåŸºçº¿å®éªŒ..."
# baseline_sac_start=$(date +%s)
# monitor_gpu "baseline_sac" 30  # å‡è®¾è¿è¡Œ30åˆ†é’Ÿ

# python train.py \
#     --policy SAC \
#     --target_exploration_steps 25_000 \
#     --start_timesteps 25_000 \
#     --max_timesteps 500_000 \
#     --evaluate_every 50_000 \
#     --training_pcb ${RL_PCB}/dataset/base/region_8_fixed.pcb \
#     --evaluation_pcb ${RL_PCB}/dataset/base/evaluation.pcb \
#     --tensorboard_dir ${EXP_DIR}/work \
#     -w 4.0 --hpwl 4.0 -o 2.0 \
#     --hyperparameters ${EXP_DIR}/hyperparameters/hp_sac_baseline.json \
#     --incremental_replay_buffer double \
#     --verbose 1 \
#     --runs 3 \
#     --experiment baseline_sac_original \
#     --device cuda \
#     --pcb_save_freq 5000

# baseline_sac_end=$(date +%s)
# log_performance "baseline_sac" $baseline_sac_start $baseline_sac_end "N/A"
# echo "âœ… SACåŸºçº¿å®éªŒå®Œæˆ"

echo ""
echo "âš¡ é˜¶æ®µ 2: æ€§èƒ½ä¼˜åŒ–æµ‹è¯• (ä¼˜åŒ–ç‰ˆæœ¬)"
echo "=========================================="

# SACä¼˜åŒ–ç‰ˆæµ‹è¯•
echo "ğŸš€ è¿è¡ŒSACä¼˜åŒ–å®éªŒ..."
optimized_sac_start=$(date +%s)
monitor_gpu "optimized_sac" 20  # é¢„æœŸæ›´å¿«å®Œæˆ

python train_optimized.py \
    --policy SAC \
    --target_exploration_steps 25_000 \
    --start_timesteps 25_000 \
    --max_timesteps 500_000 \
    --evaluate_every 50_000 \
    --training_pcb ${RL_PCB}/dataset/base/region_8_fixed.pcb \
    --evaluation_pcb ${RL_PCB}/dataset/base/evaluation.pcb \
    --tensorboard_dir ${EXP_DIR}/work \
    -w 4.0 --hpwl 4.0 -o 2.0 \
    --hyperparameters ${EXP_DIR}/hyperparameters/hp_sac_optimized.json \
    --incremental_replay_buffer double \
    --verbose 1 \
    --runs 3 \
    --experiment optimized_sac_multithread_gpu \
    --device cuda \
    --pcb_save_freq 5000 \
    --enable_multithread true \
    --enable_gpu_optimization true \
    --num_workers 6

optimized_sac_end=$(date +%s)
log_performance "optimized_sac" $optimized_sac_start $optimized_sac_end "N/A"
echo "âœ… SACä¼˜åŒ–å®éªŒå®Œæˆ"

echo ""
echo "ğŸ”¬ é˜¶æ®µ 3: æ¶ˆèå®éªŒ (åˆ†ç»„ä»¶æµ‹è¯•)"
echo "=========================================="

# ä»…å¤šçº¿ç¨‹ä¼˜åŒ–æµ‹è¯•
echo "ğŸ§µ æµ‹è¯•ä»…å¤šçº¿ç¨‹ä¼˜åŒ–..."
multithread_only_start=$(date +%s)

python train_optimized.py \
    --policy SAC \
    --target_exploration_steps 25_000 \
    --start_timesteps 25_000 \
    --max_timesteps 300_000 \
    --evaluate_every 50_000 \
    --training_pcb ${RL_PCB}/dataset/base/region_8_fixed.pcb \
    --evaluation_pcb ${RL_PCB}/dataset/base/evaluation.pcb \
    --tensorboard_dir ${EXP_DIR}/work \
    -w 4.0 --hpwl 4.0 -o 2.0 \
    --hyperparameters ${EXP_DIR}/hyperparameters/hp_sac_baseline.json \
    --incremental_replay_buffer double \
    --verbose 1 \
    --runs 2 \
    --experiment ablation_sac_multithread_only \
    --device cuda \
    --pcb_save_freq 5000 \
    --enable_multithread true \
    --enable_gpu_optimization false \
    --num_workers 6

multithread_only_end=$(date +%s)
log_performance "multithread_only" $multithread_only_start $multithread_only_end "N/A"
echo "âœ… å¤šçº¿ç¨‹å•ç‹¬æµ‹è¯•å®Œæˆ"

# ä»…GPUä¼˜åŒ–æµ‹è¯•
echo "ğŸ® æµ‹è¯•ä»…GPUä¼˜åŒ–..."
gpu_only_start=$(date +%s)

python train_optimized.py \
    --policy SAC \
    --target_exploration_steps 25_000 \
    --start_timesteps 25_000 \
    --max_timesteps 300_000 \
    --evaluate_every 50_000 \
    --training_pcb ${RL_PCB}/dataset/base/region_8_fixed.pcb \
    --evaluation_pcb ${RL_PCB}/dataset/base/evaluation.pcb \
    --tensorboard_dir ${EXP_DIR}/work \
    -w 4.0 --hpwl 4.0 -o 2.0 \
    --hyperparameters ${EXP_DIR}/hyperparameters/hp_sac_optimized.json \
    --incremental_replay_buffer double \
    --verbose 1 \
    --runs 2 \
    --experiment ablation_sac_gpu_only \
    --device cuda \
    --pcb_save_freq 5000 \
    --enable_multithread false \
    --enable_gpu_optimization true \
    --num_workers 1

gpu_only_end=$(date +%s)
log_performance "gpu_only" $gpu_only_start $gpu_only_end "N/A"
echo "âœ… GPUå•ç‹¬æµ‹è¯•å®Œæˆ"

# è¿”å›å®éªŒç›®å½•
cd ${EXP_DIR}

echo ""
echo "ğŸ“Š é˜¶æ®µ 4: ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"
echo "=========================================="

# ç”ŸæˆæŠ¥å‘Šé…ç½®
python report_config.py

# ç”Ÿæˆå®éªŒæŠ¥å‘Š
echo "ğŸ“„ ç”Ÿæˆå®éªŒæŠ¥å‘Š..."
cd ${RL_PCB}/src/report_generation

python generate_experiment_report.py \
    --dir ${EXP_DIR}/work \
    --hyperparameters ${EXP_DIR}/hyperparameters/hp_sac_baseline.json ${EXP_DIR}/hyperparameters/hp_sac_optimized.json \
    --report_config ${EXP_DIR}/report_config.json \
    --output ${EXP_DIR}/performance_optimization_report.pdf \
    -y \
    --tmp_dir ${EXP_DIR}/tmp

cd ${EXP_DIR}

echo ""
echo "ğŸ“ˆ æ€§èƒ½åˆ†æ"
echo "=========================================="

# ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
python << 'EOF'
import os
import time
from datetime import datetime

def analyze_performance_log():
    log_file = "performance_logs/performance_comparison.log"
    if not os.path.exists(log_file):
        print("âŒ æ€§èƒ½æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("ğŸ” æ€§èƒ½åˆ†æç»“æœ:")
    print("-" * 40)
    
    with open(log_file, 'r') as f:
        lines = f.readlines()
    
    # è§£ææ€§èƒ½æ•°æ®
    experiments = {}
    for line in lines:
        if " - è€—æ—¶:" in line:
            parts = line.strip().split(" - è€—æ—¶: ")
            if len(parts) == 2:
                exp_name = parts[0].split(": ")[1]
                duration = int(parts[1].split("ç§’")[0])
                experiments[exp_name] = duration
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    if 'baseline_sac' in experiments and 'optimized_sac' in experiments:
        baseline_time = experiments['baseline_sac']
        optimized_time = experiments['optimized_sac']
        speedup = baseline_time / optimized_time
        
        print(f"SACåŸºçº¿ç‰ˆæœ¬è€—æ—¶: {baseline_time}ç§’")
        print(f"SACä¼˜åŒ–ç‰ˆæœ¬è€—æ—¶: {optimized_time}ç§’")
        print(f"ğŸš€ æ€»ä½“åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"â° æ—¶é—´èŠ‚çœ: {baseline_time - optimized_time}ç§’ ({((baseline_time - optimized_time)/baseline_time)*100:.1f}%)")
    
    # åˆ†ææ¶ˆèå®éªŒç»“æœ
    if 'multithread_only' in experiments:
        mt_time = experiments['multithread_only']
        print(f"\nğŸ§µ å¤šçº¿ç¨‹ä¼˜åŒ–æ•ˆæœ:")
        if 'baseline_sac' in experiments:
            mt_speedup = (experiments['baseline_sac'] * 0.6) / mt_time  # è°ƒæ•´åŸºçº¿æ—¶é—´
            print(f"   ä»…å¤šçº¿ç¨‹åŠ é€Ÿæ¯”: {mt_speedup:.2f}x")
    
    if 'gpu_only' in experiments:
        gpu_time = experiments['gpu_only']
        print(f"\nğŸ® GPUä¼˜åŒ–æ•ˆæœ:")
        if 'baseline_sac' in experiments:
            gpu_speedup = (experiments['baseline_sac'] * 0.6) / gpu_time  # è°ƒæ•´åŸºçº¿æ—¶é—´
            print(f"   ä»…GPUä¼˜åŒ–åŠ é€Ÿæ¯”: {gpu_speedup:.2f}x")
    
    print("-" * 40)

analyze_performance_log()
EOF

echo ""
echo "ğŸ¯ å®éªŒå®Œæˆæ€»ç»“"
echo "=========================================="
echo "âœ… åŸºçº¿æ€§èƒ½æµ‹è¯•å®Œæˆ"
echo "âœ… ä¼˜åŒ–ç‰ˆæœ¬æµ‹è¯•å®Œæˆ"  
echo "âœ… æ¶ˆèå®éªŒå®Œæˆ"
echo "âœ… æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
echo ""
echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:"
echo "   - æ€§èƒ½æŠ¥å‘Š: performance_optimization_report.pdf"
echo "   - æ€§èƒ½æ—¥å¿—: performance_logs/performance_comparison.log"
echo "   - GPUç›‘æ§æ—¥å¿—: performance_logs/gpu_*.log"
echo "   - TensorBoardæ•°æ®: work/"
echo ""
echo "ğŸŒ TensorBoardåœ°å€: http://localhost:6006"
echo ""
echo "ğŸ‰ æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”å®éªŒå®Œæˆ!"

# æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
echo "$(date): æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”å®éªŒå®Œæˆ" >> $PERF_LOG