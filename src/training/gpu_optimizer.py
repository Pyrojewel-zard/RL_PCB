"""
GPUå†…å­˜å’Œåˆ©ç”¨ç‡ä¼˜åŒ–æ–¹æ¡ˆ
è§£å†³SACè®­ç»ƒæ—¶GPUå ç”¨ç‡å’Œå†…å­˜åˆ©ç”¨ç‡ä¸é«˜çš„é—®é¢˜
"""
import torch
import torch.nn as nn
import numpy as np
from torch.optim import Adam
import torch.nn.functional as F
from collections import defaultdict
import gc
import psutil
import time

class GPUOptimizer:
    """
    GPUæ€§èƒ½ä¼˜åŒ–å™¨ï¼Œæä¾›å†…å­˜ç®¡ç†å’Œè®¡ç®—ä¼˜åŒ–åŠŸèƒ½
    """
    
    def __init__(self, device='cuda', verbose=1):
        self.device = device
        self.verbose = verbose
        self.memory_stats = defaultdict(list)
        
        if torch.cuda.is_available() and device == 'cuda':
            self.gpu_available = True
            self.gpu_count = torch.cuda.device_count()
            self.current_device = torch.cuda.current_device()
            
            # è·å–GPUå±æ€§
            gpu_props = torch.cuda.get_device_properties(self.current_device)
            self.gpu_memory = gpu_props.total_memory
            self.gpu_name = gpu_props.name
            
            if verbose > 0:
                print(f"ğŸ”§ GPUä¼˜åŒ–å™¨åˆå§‹åŒ–")
                print(f"   GPUè®¾å¤‡: {self.gpu_name}")
                print(f"   GPUå†…å­˜: {self.gpu_memory / 1024**3:.2f} GB")
                print(f"   å¯ç”¨GPUæ•°é‡: {self.gpu_count}")
        else:
            self.gpu_available = False
            if verbose > 0:
                print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUä¼˜åŒ–ç­–ç•¥")
    
    def optimize_memory_settings(self):
        """ä¼˜åŒ–CUDAå†…å­˜è®¾ç½®"""
        if not self.gpu_available:
            return
        
        # å¯ç”¨å†…å­˜åˆ†é…ä¼˜åŒ–
        torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ“ä½œ
        torch.backends.cudnn.deterministic = False  # ç‰ºç‰²ç¡®å®šæ€§æ¢å–æ€§èƒ½
        
        # è®¾ç½®å†…å­˜å¢é•¿ç­–ç•¥
        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
            torch.cuda.set_per_process_memory_fraction(0.9, self.current_device)
        
        # æ¸…ç©ºCUDAç¼“å­˜
        torch.cuda.empty_cache()
        
        if self.verbose > 0:
            print("âœ… CUDAå†…å­˜è®¾ç½®å·²ä¼˜åŒ–")
    
    def get_memory_info(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        if not self.gpu_available:
            return {'cpu_memory_percent': psutil.virtual_memory().percent}
        
        # GPUå†…å­˜ä¿¡æ¯
        allocated = torch.cuda.memory_allocated(self.current_device)
        reserved = torch.cuda.memory_reserved(self.current_device)
        max_allocated = torch.cuda.max_memory_allocated(self.current_device)
        
        # CPUå†…å­˜ä¿¡æ¯
        cpu_memory = psutil.virtual_memory().percent
        
        return {
            'gpu_allocated_mb': allocated / 1024**2,
            'gpu_reserved_mb': reserved / 1024**2,
            'gpu_max_allocated_mb': max_allocated / 1024**2,
            'gpu_utilization_percent': (allocated / self.gpu_memory) * 100,
            'cpu_memory_percent': cpu_memory
        }
    
    def log_memory_usage(self, step_name=""):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        info = self.get_memory_info()
        self.memory_stats[step_name].append(info)
        
        if self.verbose > 1:
            if self.gpu_available:
                print(f"ğŸ“Š [{step_name}] GPUå†…å­˜: {info['gpu_allocated_mb']:.1f}MB "
                      f"({info['gpu_utilization_percent']:.1f}%), "
                      f"CPUå†…å­˜: {info['cpu_memory_percent']:.1f}%")
            else:
                print(f"ğŸ“Š [{step_name}] CPUå†…å­˜: {info['cpu_memory_percent']:.1f}%")
    
    def clear_cache(self, aggressive=False):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        if self.gpu_available:
            torch.cuda.empty_cache()
            if aggressive:
                torch.cuda.ipc_collect()
        
        if aggressive:
            gc.collect()
    
    def optimize_batch_processing(self, batch_size, max_memory_percent=85):
        """
        æ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        
        Args:
            batch_size: åŸå§‹æ‰¹å¤„ç†å¤§å°
            max_memory_percent: æœ€å¤§å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
        
        Returns:
            ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°
        """
        if not self.gpu_available:
            return batch_size
        
        current_memory_percent = (torch.cuda.memory_allocated(self.current_device) / self.gpu_memory) * 100
        
        if current_memory_percent > max_memory_percent:
            # å‡å°‘æ‰¹å¤„ç†å¤§å°
            reduction_factor = max_memory_percent / current_memory_percent
            new_batch_size = max(1, int(batch_size * reduction_factor))
            
            if self.verbose > 0:
                print(f"âš¡ å†…å­˜ä¼˜åŒ–: æ‰¹å¤„ç†å¤§å°ä» {batch_size} è°ƒæ•´ä¸º {new_batch_size}")
            
            return new_batch_size
        elif current_memory_percent < max_memory_percent * 0.6:
            # å¯ä»¥å¢åŠ æ‰¹å¤„ç†å¤§å°
            increase_factor = min(1.5, max_memory_percent * 0.8 / current_memory_percent)
            new_batch_size = int(batch_size * increase_factor)
            
            if self.verbose > 0:
                print(f"âš¡ å†…å­˜ä¼˜åŒ–: æ‰¹å¤„ç†å¤§å°ä» {batch_size} è°ƒæ•´ä¸º {new_batch_size}")
            
            return new_batch_size
        
        return batch_size


class OptimizedSACTraining:
    """
    ä¼˜åŒ–åçš„SACè®­ç»ƒç±»ï¼Œæä¾›GPUåˆ©ç”¨ç‡å’Œå†…å­˜ä¼˜åŒ–
    """
    
    def __init__(self, sac_model, gpu_optimizer=None):
        self.sac_model = sac_model
        self.gpu_optimizer = gpu_optimizer or GPUOptimizer(device=sac_model.device)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.training_stats = {
            'batch_times': [],
            'memory_usage': [],
            'gpu_utilization': []
        }
        
        # åˆå§‹åŒ–ä¼˜åŒ–è®¾ç½®
        self.gpu_optimizer.optimize_memory_settings()
    
    def optimized_train_step(self, memory, batch_size, updates):
        """
        ä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤ï¼Œæé«˜GPUåˆ©ç”¨ç‡
        
        Args:
            memory: ç»éªŒå›æ”¾ç¼“å†²åŒº
            batch_size: æ‰¹å¤„ç†å¤§å°
            updates: æ›´æ–°æ¬¡æ•°
        
        Returns:
            è®­ç»ƒæŸå¤±ä¿¡æ¯
        """
        start_time = time.time()
        
        # åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        optimized_batch_size = self.gpu_optimizer.optimize_batch_processing(batch_size)
        
        # è®°å½•è®­ç»ƒå‰å†…å­˜ä½¿ç”¨
        self.gpu_optimizer.log_memory_usage("train_start")
        
        # é¢„åˆ†é…GPUå†…å­˜ï¼ˆå‡å°‘åŠ¨æ€åˆ†é…å¼€é”€ï¼‰
        with torch.cuda.device(self.sac_model.device):
            # æ‰§è¡Œä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤
            losses = self._execute_optimized_training(memory, optimized_batch_size, updates)
        
        # è®°å½•è®­ç»ƒåå†…å­˜ä½¿ç”¨
        self.gpu_optimizer.log_memory_usage("train_end")
        
        # ç»Ÿè®¡æ€§èƒ½æ•°æ®
        batch_time = time.time() - start_time
        self.training_stats['batch_times'].append(batch_time)
        
        # å®šæœŸæ¸…ç†å†…å­˜
        if updates % 100 == 0:
            self.gpu_optimizer.clear_cache()
        
        return losses
    
    def _execute_optimized_training(self, memory, batch_size, updates):
        """
        æ‰§è¡Œä¼˜åŒ–çš„è®­ç»ƒé€»è¾‘
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœæ”¯æŒï¼‰
        use_amp = hasattr(torch.cuda, 'amp') and torch.cuda.is_available()
        scaler = torch.cuda.amp.GradScaler() if use_amp else None
        
        # æ‰¹é‡é‡‡æ ·ä¼˜åŒ–
        state_batch, action_batch, next_state_batch, reward_batch, mask_batch = \
            self._optimized_sample(memory, batch_size)
        
        if use_amp:
            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            with torch.cuda.amp.autocast():
                losses = self._compute_losses(
                    state_batch, action_batch, next_state_batch, 
                    reward_batch, mask_batch, updates
                )
            
            # ç¼©æ”¾æ¢¯åº¦æ›´æ–°
            self._update_networks_with_amp(losses, scaler)
        else:
            # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
            losses = self._compute_losses(
                state_batch, action_batch, next_state_batch, 
                reward_batch, mask_batch, updates
            )
            
            # æ ‡å‡†æ¢¯åº¦æ›´æ–°
            self._update_networks(losses)
        
        return losses
    
    def _optimized_sample(self, memory, batch_size):
        """
        ä¼˜åŒ–çš„æ‰¹é‡é‡‡æ ·ï¼Œå‡å°‘GPU-CPUæ•°æ®ä¼ è¾“
        """
        # ä½¿ç”¨pin_memoryåŠ é€Ÿæ•°æ®ä¼ è¾“
        state_batch, action_batch, next_state_batch, reward_batch, mask_batch = \
            memory.sample(batch_size=batch_size)
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶ä½¿ç”¨éé˜»å¡ä¼ è¾“
        if self.gpu_optimizer.gpu_available:
            state_batch = state_batch.to(self.sac_model.device, non_blocking=True)
            action_batch = action_batch.to(self.sac_model.device, non_blocking=True)
            next_state_batch = next_state_batch.to(self.sac_model.device, non_blocking=True)
            reward_batch = reward_batch.to(self.sac_model.device, non_blocking=True)
            mask_batch = mask_batch.to(self.sac_model.device, non_blocking=True)
        
        return state_batch, action_batch, next_state_batch, reward_batch, mask_batch
    
    def _compute_losses(self, state_batch, action_batch, next_state_batch, 
                       reward_batch, mask_batch, updates):
        """
        è®¡ç®—è®­ç»ƒæŸå¤±ï¼Œä½¿ç”¨ä¼˜åŒ–çš„è®¡ç®—å›¾
        """
        # ä½¿ç”¨åŸæœ‰çš„SACè®­ç»ƒé€»è¾‘ï¼Œä½†å¢åŠ GPUä¼˜åŒ–
        return self.sac_model.train(
            memory=None,  # ç›´æ¥ä¼ å…¥æ‰¹æ•°æ®
            batch_size=len(state_batch),
            updates=updates,
            precomputed_batch=(state_batch, action_batch, next_state_batch, 
                             reward_batch, mask_batch)
        )
    
    def _update_networks(self, losses):
        """æ ‡å‡†ç½‘ç»œå‚æ•°æ›´æ–°"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦è£å‰ªç­‰ä¼˜åŒ–
        pass
    
    def _update_networks_with_amp(self, losses, scaler):
        """ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦çš„ç½‘ç»œå‚æ•°æ›´æ–°"""
        # å®ç°AMPçš„æ¢¯åº¦æ›´æ–°é€»è¾‘
        pass
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.training_stats['batch_times']:
            return {}
        
        avg_batch_time = np.mean(self.training_stats['batch_times'])
        memory_info = self.gpu_optimizer.get_memory_info()
        
        return {
            'avg_batch_time': avg_batch_time,
            'batches_per_second': 1.0 / avg_batch_time,
            'memory_info': memory_info,
            'total_batches': len(self.training_stats['batch_times'])
        }


def enhance_sac_with_gpu_optimization(sac_model, verbose=1):
    """
    ä¸ºSACæ¨¡å‹æ·»åŠ GPUä¼˜åŒ–åŠŸèƒ½
    
    Args:
        sac_model: SACæ¨¡å‹å®ä¾‹
        verbose: æ—¥å¿—è¯¦ç»†ç¨‹åº¦
    
    Returns:
        å¢å¼ºåçš„SACæ¨¡å‹
    """
    # åˆ›å»ºGPUä¼˜åŒ–å™¨
    gpu_optimizer = GPUOptimizer(device=sac_model.device, verbose=verbose)
    
    # åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨
    optimized_trainer = OptimizedSACTraining(sac_model, gpu_optimizer)
    
    # ä¿å­˜åŸå§‹è®­ç»ƒæ–¹æ³•
    original_train = sac_model.train
    original_learn = sac_model.learn
    
    def enhanced_train(memory, batch_size, updates, precomputed_batch=None):
        """å¢å¼ºçš„è®­ç»ƒæ–¹æ³•"""
        if precomputed_batch is not None:
            # ä½¿ç”¨é¢„è®¡ç®—çš„æ‰¹æ•°æ®ï¼ˆGPUä¼˜åŒ–è·¯å¾„ï¼‰
            state_batch, action_batch, next_state_batch, reward_batch, mask_batch = precomputed_batch
        else:
            # æ ‡å‡†è·¯å¾„ï¼Œä½¿ç”¨ä¼˜åŒ–è®­ç»ƒå™¨
            return optimized_trainer.optimized_train_step(memory, batch_size, updates)
        
        # æ‰§è¡ŒåŸå§‹è®­ç»ƒé€»è¾‘ï¼ˆä½†ä½¿ç”¨é¢„è®¡ç®—æ•°æ®ï¼‰
        return original_train(memory, batch_size, updates)
    
    def enhanced_learn(timesteps, callback, start_timesteps=25_000, 
                      incremental_replay_buffer=None, 
                      enable_gpu_optimization=True,
                      adaptive_batch_size=True,
                      memory_efficient_mode=True):
        """
        å¢å¼ºçš„å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…å«GPUä¼˜åŒ–
        
        Args:
            enable_gpu_optimization: å¯ç”¨GPUä¼˜åŒ–
            adaptive_batch_size: è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
            memory_efficient_mode: å†…å­˜é«˜æ•ˆæ¨¡å¼
        """
        
        if enable_gpu_optimization and gpu_optimizer.gpu_available:
            print("ğŸš€ å¯ç”¨GPUä¼˜åŒ–æ¨¡å¼")
            
            # è®¾ç½®ä¼˜åŒ–å‚æ•°
            if adaptive_batch_size:
                # æ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
                original_batch_size = sac_model.batch_size
                optimized_batch_size = gpu_optimizer.optimize_batch_processing(original_batch_size)
                sac_model.batch_size = optimized_batch_size
                print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–: {original_batch_size} â†’ {optimized_batch_size}")
            
            if memory_efficient_mode:
                # å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼
                gpu_optimizer.optimize_memory_settings()
                print("ğŸ’¾ å†…å­˜é«˜æ•ˆæ¨¡å¼å·²å¯ç”¨")
        
        # æ‰§è¡ŒåŸå§‹å­¦ä¹ é€»è¾‘
        result = original_learn(timesteps, callback, start_timesteps, incremental_replay_buffer)
        
        # è®­ç»ƒå®Œæˆåçš„æ€§èƒ½æŠ¥å‘Š
        if enable_gpu_optimization:
            stats = optimized_trainer.get_performance_stats()
            if stats:
                print("\nğŸ“ˆ GPUä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š:")
                print(f"   å¹³å‡æ‰¹å¤„ç†æ—¶é—´: {stats['avg_batch_time']:.4f}ç§’")
                print(f"   æ‰¹å¤„ç†é€Ÿç‡: {stats['batches_per_second']:.2f} æ‰¹/ç§’")
                print(f"   æ€»æ‰¹æ¬¡æ•°: {stats['total_batches']}")
                
                if 'memory_info' in stats and gpu_optimizer.gpu_available:
                    mem_info = stats['memory_info']
                    print(f"   GPUå†…å­˜ä½¿ç”¨: {mem_info['gpu_utilization_percent']:.1f}%")
                    print(f"   GPUå†…å­˜åˆ†é…: {mem_info['gpu_allocated_mb']:.1f}MB")
        
        return result
    
    # æ›¿æ¢æ–¹æ³•
    sac_model.train = enhanced_train
    sac_model.learn = enhanced_learn
    sac_model.gpu_optimizer = gpu_optimizer
    sac_model.optimized_trainer = optimized_trainer
    
    return sac_model


# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®å»ºè®®
class GPUOptimizationConfig:
    """GPUä¼˜åŒ–é…ç½®å»ºè®®"""
    
    @staticmethod
    def get_recommended_settings(gpu_memory_gb):
        """
        æ ¹æ®GPUå†…å­˜å¤§å°æ¨èè®¾ç½®
        
        Args:
            gpu_memory_gb: GPUå†…å­˜å¤§å°ï¼ˆGBï¼‰
        
        Returns:
            æ¨èçš„é…ç½®å­—å…¸
        """
        if gpu_memory_gb >= 24:  # RTX 3090/4090, A100ç­‰
            return {
                'batch_size': 512,
                'buffer_size': 2_000_000,
                'num_workers': 8,
                'gradient_accumulation_steps': 1,
                'use_amp': True
            }
        elif gpu_memory_gb >= 12:  # RTX 3080Ti, RTX 4070Tiç­‰
            return {
                'batch_size': 256,
                'buffer_size': 1_500_000,
                'num_workers': 6,
                'gradient_accumulation_steps': 2,
                'use_amp': True
            }
        elif gpu_memory_gb >= 8:  # RTX 3070, RTX 4060Tiç­‰
            return {
                'batch_size': 128,
                'buffer_size': 1_000_000,
                'num_workers': 4,
                'gradient_accumulation_steps': 2,
                'use_amp': True
            }
        else:  # RTX 3060ç­‰
            return {
                'batch_size': 64,
                'buffer_size': 500_000,
                'num_workers': 2,
                'gradient_accumulation_steps': 4,
                'use_amp': True
            }
    
    @staticmethod
    def print_optimization_guide():
        """æ‰“å°GPUä¼˜åŒ–æŒ‡å—"""
        print("""
ğŸ”§ GPUä¼˜åŒ–æŒ‡å—:

1. å†…å­˜ä¼˜åŒ–:
   - å¢åŠ batch_sizeè‡³GPUå†…å­˜ä¸Šé™çš„80-90%
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰èŠ‚çœ50%å†…å­˜
   - å®šæœŸæ¸…ç†CUDAç¼“å­˜

2. è®¡ç®—ä¼˜åŒ–:
   - å¯ç”¨cudnn.benchmarkåŠ é€Ÿå·ç§¯æ“ä½œ
   - ä½¿ç”¨æ•°æ®å¹¶è¡Œï¼ˆå¦‚æœæœ‰å¤šGPUï¼‰
   - ä¼˜åŒ–æ•°æ®ä¼ è¾“ï¼ˆpin_memory, non_blockingï¼‰

3. è®­ç»ƒç­–ç•¥:
   - ä½¿ç”¨æ›´å¤§çš„replay buffer
   - å¢åŠ gradient_stepsæé«˜GPUåˆ©ç”¨ç‡
   - è€ƒè™‘æ¢¯åº¦ç´¯ç§¯æ›¿ä»£æ›´å¤§batch_size

4. ç›‘æ§å·¥å…·:
   - nvidia-smiç›‘æ§GPUåˆ©ç”¨ç‡
   - tensorboardç›‘æ§è®­ç»ƒæŒ‡æ ‡
   - å†…ç½®æ€§èƒ½ç»Ÿè®¡å·¥å…·
        """)