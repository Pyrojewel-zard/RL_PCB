"""
æ€§èƒ½ä¼˜åŒ–é›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åŒæ—¶åº”ç”¨å¤šçº¿ç¨‹å’ŒGPUä¼˜åŒ–æ¥æå‡RL_PCBè®­ç»ƒæ€§èƒ½
"""

import os
import sys
import torch
import numpy as np
from multithread_explorer import ThreadSafeExplorer, add_multithread_support_to_sac
from gpu_optimizer import enhance_sac_with_gpu_optimization, GPUOptimizationConfig
from SAC import SAC


class PerformanceOptimizedSAC(SAC):
    """
    æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬çš„SACï¼Œé›†æˆå¤šçº¿ç¨‹æ¢ç´¢å’ŒGPUä¼˜åŒ–
    """
    
    def __init__(self, *args, **kwargs):
        # æå–ä¼˜åŒ–ç›¸å…³å‚æ•°
        self.enable_multithread = kwargs.pop('enable_multithread', True)
        self.enable_gpu_optimization = kwargs.pop('enable_gpu_optimization', True)
        self.num_workers = kwargs.pop('num_workers', 4)
        self.verbose = kwargs.pop('verbose', 1)
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(*args, **kwargs)
        
        # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
        self._apply_optimizations()
    
    def _apply_optimizations(self):
        """åº”ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–"""
        
        if self.verbose > 0:
            print("ğŸ”§ æ­£åœ¨åº”ç”¨æ€§èƒ½ä¼˜åŒ–...")
        
        # 1. åº”ç”¨GPUä¼˜åŒ–
        if self.enable_gpu_optimization:
            self._apply_gpu_optimization()
        
        # 2. åº”ç”¨å¤šçº¿ç¨‹ä¼˜åŒ–
        if self.enable_multithread:
            self._apply_multithread_optimization()
        
        # 3. æ ¹æ®ç¡¬ä»¶é…ç½®ä¼˜åŒ–è¶…å‚æ•°
        self._auto_tune_hyperparameters()
        
        if self.verbose > 0:
            print("âœ… æ€§èƒ½ä¼˜åŒ–åº”ç”¨å®Œæˆ!")
    
    def _apply_gpu_optimization(self):
        """åº”ç”¨GPUä¼˜åŒ–"""
        try:
            enhance_sac_with_gpu_optimization(self, verbose=self.verbose)
            if self.verbose > 0:
                print("âœ… GPUä¼˜åŒ–å·²å¯ç”¨")
        except Exception as e:
            if self.verbose > 0:
                print(f"âš ï¸ GPUä¼˜åŒ–å¯ç”¨å¤±è´¥: {e}")
    
    def _apply_multithread_optimization(self):
        """åº”ç”¨å¤šçº¿ç¨‹ä¼˜åŒ–"""
        try:
            # åˆ›å»ºå¤šçº¿ç¨‹æ¢ç´¢å™¨
            self.explorer = ThreadSafeExplorer(self, num_workers=self.num_workers, verbose=self.verbose)
            self.env_lock = torch.multiprocessing.Lock() if hasattr(torch, 'multiprocessing') else None
            
            if self.verbose > 0:
                print(f"âœ… å¤šçº¿ç¨‹ä¼˜åŒ–å·²å¯ç”¨ ({self.num_workers} çº¿ç¨‹)")
        except Exception as e:
            if self.verbose > 0:
                print(f"âš ï¸ å¤šçº¿ç¨‹ä¼˜åŒ–å¯ç”¨å¤±è´¥: {e}")
    
    def _auto_tune_hyperparameters(self):
        """æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è°ƒä¼˜è¶…å‚æ•°"""
        if not torch.cuda.is_available():
            return
        
        try:
            # è·å–GPUå†…å­˜å¤§å°
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            # è·å–æ¨èè®¾ç½®
            recommended = GPUOptimizationConfig.get_recommended_settings(gpu_memory_gb)
            
            # åº”ç”¨æ¨èè®¾ç½®ï¼ˆå¦‚æœå½“å‰å€¼è¾ƒå°ï¼‰
            if hasattr(self, 'batch_size') and self.batch_size < recommended['batch_size']:
                old_batch_size = self.batch_size
                self.batch_size = recommended['batch_size']
                if self.verbose > 0:
                    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–: {old_batch_size} â†’ {self.batch_size}")
            
            if hasattr(self, 'buffer_size') and self.buffer_size < recommended['buffer_size']:
                old_buffer_size = self.buffer_size
                self.buffer_size = recommended['buffer_size']
                if self.verbose > 0:
                    print(f"ğŸ’¾ ç¼“å†²åŒºå¤§å°ä¼˜åŒ–: {old_buffer_size:,} â†’ {self.buffer_size:,}")
            
            # æ›´æ–°å·¥ä½œçº¿ç¨‹æ•°é‡
            if hasattr(self, 'explorer') and self.explorer.num_workers < recommended['num_workers']:
                old_workers = self.explorer.num_workers
                self.explorer.num_workers = recommended['num_workers']
                self.num_workers = recommended['num_workers']
                if self.verbose > 0:
                    print(f"ğŸ”„ å·¥ä½œçº¿ç¨‹æ•°ä¼˜åŒ–: {old_workers} â†’ {self.num_workers}")
                    
        except Exception as e:
            if self.verbose > 0:
                print(f"âš ï¸ è‡ªåŠ¨è°ƒä¼˜å¤±è´¥: {e}")
    
    def explore_for_expert_targets(self,
                                 reward_target_exploration_steps=25_000,
                                 output_dir=None,
                                 save_pcb_every_n_steps=1000,
                                 use_multithread=None):
        """
        ä¼˜åŒ–ç‰ˆæœ¬çš„ä¸“å®¶ç›®æ ‡æ¢ç´¢
        
        Args:
            reward_target_exploration_steps: æ¢ç´¢æ­¥æ•°
            output_dir: è¾“å‡ºç›®å½•
            save_pcb_every_n_steps: PCBä¿å­˜é¢‘ç‡
            use_multithread: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤è®¾ç½®ï¼‰
        """
        if use_multithread is None:
            use_multithread = self.enable_multithread and hasattr(self, 'explorer')
        
        if use_multithread:
            # ä½¿ç”¨å¤šçº¿ç¨‹æ¢ç´¢
            if self.verbose > 0:
                print(f"ğŸš€ å¯åŠ¨å¤šçº¿ç¨‹æ¢ç´¢ ({self.num_workers} çº¿ç¨‹)")
            
            stats = self.explorer.explore(
                total_steps=reward_target_exploration_steps,
                output_dir=output_dir,
                save_pcb_every_n_steps=save_pcb_every_n_steps
            )
            
            if self.verbose > 0:
                print(f"âœ… å¤šçº¿ç¨‹æ¢ç´¢å®Œæˆ! æ€§èƒ½æå‡: {stats['steps_per_second']:.2f} æ­¥/ç§’")
        else:
            # ä½¿ç”¨åŸå§‹å•çº¿ç¨‹æ¢ç´¢
            if self.verbose > 0:
                print("ğŸŒ ä½¿ç”¨å•çº¿ç¨‹æ¢ç´¢")
            
            super().explore_for_expert_targets(
                reward_target_exploration_steps=reward_target_exploration_steps,
                output_dir=output_dir,
                save_pcb_every_n_steps=save_pcb_every_n_steps
            )
    
    def get_performance_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {
            'optimizations_enabled': {
                'multithread': self.enable_multithread,
                'gpu_optimization': self.enable_gpu_optimization
            },
            'hardware_info': {},
            'configuration': {
                'num_workers': getattr(self, 'num_workers', 'N/A'),
                'batch_size': getattr(self, 'batch_size', 'N/A'),
                'buffer_size': getattr(self, 'buffer_size', 'N/A')
            }
        }
        
        # è·å–ç¡¬ä»¶ä¿¡æ¯
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            report['hardware_info']['gpu'] = {
                'name': gpu_props.name,
                'memory_gb': gpu_props.total_memory / (1024**3),
                'compute_capability': f"{gpu_props.major}.{gpu_props.minor}"
            }
        
        # è·å–CPUä¿¡æ¯
        try:
            import psutil
            report['hardware_info']['cpu'] = {
                'cores': psutil.cpu_count(logical=False),
                'threads': psutil.cpu_count(logical=True),
                'memory_gb': psutil.virtual_memory().total / (1024**3)
            }
        except ImportError:
            pass
        
        # è·å–GPUä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self, 'optimized_trainer'):
            gpu_stats = self.optimized_trainer.get_performance_stats()
            report['gpu_performance'] = gpu_stats
        
        return report
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        report = self.get_performance_report()
        
        print("\n" + "="*60)
        print("ğŸš€ RL_PCB æ€§èƒ½ä¼˜åŒ–æ€»ç»“")
        print("="*60)
        
        # ä¼˜åŒ–çŠ¶æ€
        print("ğŸ“‹ ä¼˜åŒ–çŠ¶æ€:")
        for opt_name, enabled in report['optimizations_enabled'].items():
            status = "âœ… å·²å¯ç”¨" if enabled else "âŒ æœªå¯ç”¨"
            print(f"   {opt_name}: {status}")
        
        # ç¡¬ä»¶ä¿¡æ¯
        if 'hardware_info' in report:
            print("\nğŸ”§ ç¡¬ä»¶ä¿¡æ¯:")
            if 'gpu' in report['hardware_info']:
                gpu_info = report['hardware_info']['gpu']
                print(f"   GPU: {gpu_info['name']} ({gpu_info['memory_gb']:.1f}GB)")
            if 'cpu' in report['hardware_info']:
                cpu_info = report['hardware_info']['cpu']
                print(f"   CPU: {cpu_info['cores']}æ ¸/{cpu_info['threads']}çº¿ç¨‹ "
                      f"({cpu_info['memory_gb']:.1f}GB RAM)")
        
        # é…ç½®ä¿¡æ¯
        print("\nâš™ï¸ ä¼˜åŒ–é…ç½®:")
        config = report['configuration']
        for key, value in config.items():
            if value != 'N/A':
                if isinstance(value, int) and value > 1000:
                    print(f"   {key}: {value:,}")
                else:
                    print(f"   {key}: {value}")
        
        # GPUæ€§èƒ½ç»Ÿè®¡
        if 'gpu_performance' in report and report['gpu_performance']:
            print("\nğŸ“Š GPUæ€§èƒ½ç»Ÿè®¡:")
            gpu_perf = report['gpu_performance']
            if 'avg_batch_time' in gpu_perf:
                print(f"   å¹³å‡æ‰¹å¤„ç†æ—¶é—´: {gpu_perf['avg_batch_time']:.4f}ç§’")
            if 'batches_per_second' in gpu_perf:
                print(f"   æ‰¹å¤„ç†é€Ÿç‡: {gpu_perf['batches_per_second']:.2f} æ‰¹/ç§’")
        
        print("="*60)


def create_optimized_sac_model(train_env, 
                              hyperparameters,
                              device='cuda',
                              enable_multithread=True,
                              enable_gpu_optimization=True,
                              num_workers=None,
                              verbose=1):
    """
    åˆ›å»ºæ€§èƒ½ä¼˜åŒ–çš„SACæ¨¡å‹
    
    Args:
        train_env: è®­ç»ƒç¯å¢ƒ
        hyperparameters: è¶…å‚æ•°
        device: è®¾å¤‡
        enable_multithread: æ˜¯å¦å¯ç”¨å¤šçº¿ç¨‹
        enable_gpu_optimization: æ˜¯å¦å¯ç”¨GPUä¼˜åŒ–
        num_workers: å·¥ä½œçº¿ç¨‹æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©ï¼‰
        verbose: æ—¥å¿—è¯¦ç»†ç¨‹åº¦
    
    Returns:
        ä¼˜åŒ–åçš„SACæ¨¡å‹
    """
    
    # è‡ªåŠ¨é€‰æ‹©å·¥ä½œçº¿ç¨‹æ•°
    if num_workers is None:
        try:
            import psutil
            cpu_cores = psutil.cpu_count(logical=False)
            num_workers = min(max(2, cpu_cores // 2), 8)  # 2-8ä¸ªçº¿ç¨‹
        except ImportError:
            num_workers = 4
    
    # åˆ›å»ºä¼˜åŒ–çš„SACæ¨¡å‹
    model = PerformanceOptimizedSAC(
        train_env=train_env,
        hyperparameters=hyperparameters,
        device=device,
        enable_multithread=enable_multithread,
        enable_gpu_optimization=enable_gpu_optimization,
        num_workers=num_workers,
        verbose=verbose
    )
    
    if verbose > 0:
        print(f"\nğŸ¯ æ€§èƒ½ä¼˜åŒ–SACæ¨¡å‹åˆ›å»ºå®Œæˆ!")
        model.print_performance_summary()
    
    return model


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    print("ğŸš€ RL_PCB æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹")
    print("-" * 40)
    
    # å‡è®¾å·²æœ‰ç¯å¢ƒå’Œè¶…å‚æ•°
    # train_env = your_training_environment
    # hyperparameters = your_hyperparameters
    
    # åˆ›å»ºä¼˜åŒ–æ¨¡å‹
    # model = create_optimized_sac_model(
    #     train_env=train_env,
    #     hyperparameters=hyperparameters,
    #     device='cuda',
    #     enable_multithread=True,
    #     enable_gpu_optimization=True,
    #     num_workers=6,
    #     verbose=1
    # )
    
    # æ‰§è¡Œä¼˜åŒ–çš„ä¸“å®¶ç›®æ ‡æ¢ç´¢
    # model.explore_for_expert_targets(
    #     reward_target_exploration_steps=50_000,
    #     output_dir="./optimized_output",
    #     save_pcb_every_n_steps=1000
    # )
    
    # æ‰§è¡Œä¼˜åŒ–çš„è®­ç»ƒ
    # model.learn(
    #     timesteps=1_000_000,
    #     callback=your_callback,
    #     enable_gpu_optimization=True,
    #     adaptive_batch_size=True,
    #     memory_efficient_mode=True
    # )
    
    # è·å–æ€§èƒ½æŠ¥å‘Š
    # model.print_performance_summary()
    
    # æ‰“å°ä¼˜åŒ–æŒ‡å—
    GPUOptimizationConfig.print_optimization_guide()


if __name__ == "__main__":
    example_usage()