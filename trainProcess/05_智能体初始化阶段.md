# 05. 智能体初始化阶段

## 概述
智能体初始化阶段为每个未放置的PCB组件创建独立的智能体，每个智能体负责学习该组件的最优放置位置和方向。

## 详细流程

### 5.1 智能体参数配置

#### 5.1.1 智能体参数结构
```python
agent_params = agent_parameters({
    # 环境信息
    "board": self.b,                    # 电路板对象
    "graph": self.g,                    # 图形对象
    "board_width": self.b.get_width(),  # 电路板宽度
    "board_height": self.b.get_height(), # 电路板高度
    
    # 组件信息
    "node": nn[i],                      # 当前组件节点
    "neighbors": neighbors,              # 邻居组件列表
    "eoi": eoi,                        # 相关边列表
    
    # 网络配置
    "net": self.parameters.net,         # 神经网络路径
    "step_size": 1.0,                  # 步长
    "max_steps": self.parameters.max_steps, # 最大步数
    
    # 优化目标
    "opt_euclidean_distance": True,     # 优化欧几里得距离
    "opt_hpwl": True,                  # 优化HPWL
    
    # 随机种子
    "seed": self.parameters.seed,
    
    # 连接信息
    "nets": nets,                      # 网络列表
    
    # 动作空间
    "max_action": self.parameters.agent_max_action, # 最大动作值
    "expl_noise": self.parameters.agent_expl_noise, # 探索噪声
    
    # 奖励权重
    "n": self.parameters.n,            # 欧几里得距离权重
    "m": self.parameters.m,            # 重叠惩罚权重
    "p": self.parameters.p,            # HPWL权重
    
    # 其他配置
    "ignore_power": self.parameters.ignore_power, # 忽略电源网络
    "log_file": self.parameters.log_dir # 日志文件路径
})
```

### 5.2 智能体构造函数

#### 5.2.1 智能体初始化
```python
class agent:
    def __init__(self, parameters):
        self.parameters = parameters
        
        # 初始化组件信息
        self.node = parameters.node
        self.neighbors = parameters.neighbors
        self.eoi = parameters.eoi
        
        # 设置动作空间
        self.action_space = self._setup_action_space()
        
        # 设置观察空间
        self.observation_space = self._setup_observation_space()
        
        # 初始化神经网络
        self.policy_net = self._setup_policy_network()
        self.value_net = self._setup_value_network()
        
        # 初始化优化器
        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=parameters.learning_rate)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=parameters.learning_rate)
        
        # 初始化经验回放缓冲区
        self.replay_buffer = ReplayBuffer(parameters.buffer_size)
        
        # 设置随机数生成器
        self.rng = np.random.default_rng(seed=parameters.seed)
```

### 5.3 动作空间设置

#### 5.3.1 动作空间定义
```python
def _setup_action_space(self):
    """设置动作空间"""
    # 动作包括：X位移、Y位移、旋转角度
    action_dim = 3
    
    # 动作范围
    max_translation = min(self.parameters.board_width, self.parameters.board_height) * 0.1
    max_rotation = 360.0  # 度
    
    action_space = {
        'translation_x': [-max_translation, max_translation],
        'translation_y': [-max_translation, max_translation],
        'rotation': [-max_rotation, max_rotation]
    }
    
    return action_space
```

#### 5.3.2 动作执行
```python
def execute_action(self, action):
    """执行动作"""
    # 解析动作
    translation_x = action[0] * self.action_space['translation_x'][1]
    translation_y = action[1] * self.action_space['translation_y'][1]
    rotation = action[2] * self.action_space['rotation'][1]
    
    # 获取当前位置
    current_pos = self.node.get_pos()
    current_orientation = self.node.get_orientation()
    
    # 计算新位置
    new_x = current_pos[0] + translation_x
    new_y = current_pos[1] + translation_y
    new_orientation = (current_orientation + rotation) % 360
    
    # 边界检查
    new_x = np.clip(new_x, 0, self.parameters.board_width - self.node.get_size()[0])
    new_y = np.clip(new_y, 0, self.parameters.board_height - self.node.get_size()[1])
    
    # 更新组件位置
    self.node.set_pos([new_x, new_y])
    self.node.set_orientation(new_orientation)
```

### 5.4 观察空间设置

#### 5.4.1 观察空间定义
```python
def _setup_observation_space(self):
    """设置观察空间"""
    # 观察包括：
    # 1. 组件自身信息（位置、大小、方向）
    # 2. 邻居组件信息
    # 3. 连接信息
    # 4. 电路板边界信息
    
    obs_dim = (
        4 +  # 组件自身：x, y, width, height
        len(self.neighbors) * 4 +  # 邻居组件信息
        len(self.eoi) * 2 +  # 连接信息
        4  # 电路板边界
    )
    
    return obs_dim
```

#### 5.4.2 观察获取
```python
def get_observation(self):
    """获取当前观察"""
    obs = []
    
    # 组件自身信息
    pos = self.node.get_pos()
    size = self.node.get_size()
    obs.extend([pos[0], pos[1], size[0], size[1]])
    
    # 邻居组件信息
    for neighbor in self.neighbors:
        n_pos = neighbor.get_pos()
        n_size = neighbor.get_size()
        obs.extend([n_pos[0], n_pos[1], n_size[0], n_size[1]])
    
    # 连接信息
    for edge in self.eoi:
        obs.extend([edge.get_net_id(), edge.get_power_rail()])
    
    # 电路板边界
    obs.extend([
        self.parameters.board_width,
        self.parameters.board_height,
        0,  # 边界最小X
        0   # 边界最小Y
    ])
    
    return np.array(obs, dtype=np.float32)
```

### 5.5 神经网络设置

#### 5.5.1 策略网络
```python
def _setup_policy_network(self):
    """设置策略网络"""
    policy_net = nn.Sequential(
        nn.Linear(self.observation_space, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 3),  # 3个动作：x, y, rotation
        nn.Tanh()  # 输出范围[-1, 1]
    )
    
    return policy_net
```

#### 5.5.2 价值网络
```python
def _setup_value_network(self):
    """设置价值网络"""
    value_net = nn.Sequential(
        nn.Linear(self.observation_space + 3, 256),  # 观察 + 动作
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 1)  # 价值函数
    )
    
    return value_net
```

### 5.6 奖励函数设置

#### 5.6.1 奖励计算
```python
def calculate_reward(self):
    """计算奖励"""
    reward = 0.0
    
    # 1. 欧几里得距离奖励
    euclidean_distance = self._calculate_euclidean_distance()
    reward += self.parameters.n * euclidean_distance
    
    # 2. HPWL奖励
    hpwl = self._calculate_hpwl()
    reward += self.parameters.p * hpwl
    
    # 3. 重叠惩罚
    overlap_penalty = self._calculate_overlap_penalty()
    reward += self.parameters.m * overlap_penalty
    
    return reward
```

#### 5.6.2 欧几里得距离计算
```python
def _calculate_euclidean_distance(self):
    """计算到最优位置的欧几里得距离"""
    current_pos = self.node.get_pos()
    optimal_pos = self.node.get_optimal_pos()
    
    distance = np.sqrt(
        (current_pos[0] - optimal_pos[0])**2 + 
        (current_pos[1] - optimal_pos[1])**2
    )
    
    return -distance  # 负值，距离越小奖励越高
```

#### 5.6.3 HPWL计算
```python
def _calculate_hpwl(self):
    """计算HPWL（Half-Perimeter Wirelength）"""
    hpwl = 0.0
    
    for edge in self.eoi:
        # 获取连接的两个组件
        node1_id = edge.get_instance_id(0)
        node2_id = edge.get_instance_id(1)
        
        # 计算连接长度
        if node1_id == self.node.get_id():
            other_node = self.graph.get_node_by_id(node2_id)
        else:
            other_node = self.graph.get_node_by_id(node1_id)
        
        # 计算曼哈顿距离
        pos1 = self.node.get_pos()
        pos2 = other_node.get_pos()
        
        wirelength = abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
        hpwl += wirelength
    
    return -hpwl  # 负值，线长越短奖励越高
```

#### 5.6.4 重叠惩罚计算
```python
def _calculate_overlap_penalty(self):
    """计算与其他组件的重叠惩罚"""
    overlap_penalty = 0.0
    current_pos = self.node.get_pos()
    current_size = self.node.get_size()
    
    for neighbor in self.neighbors:
        neighbor_pos = neighbor.get_pos()
        neighbor_size = neighbor.get_size()
        
        # 计算重叠面积
        overlap_x = max(0, min(current_pos[0] + current_size[0], 
                               neighbor_pos[0] + neighbor_size[0]) - 
                               max(current_pos[0], neighbor_pos[0]))
        overlap_y = max(0, min(current_pos[1] + current_size[1], 
                               neighbor_pos[1] + neighbor_size[1]) - 
                               max(current_pos[1], neighbor_pos[1]))
        
        overlap_area = overlap_x * overlap_y
        overlap_penalty += overlap_area
    
    return overlap_penalty  # 正值，重叠越多惩罚越大
```

### 5.7 智能体重置

#### 5.7.1 重置函数
```python
def reset(self):
    """重置智能体状态"""
    # 随机初始化位置
    self.init_random()
    
    # 重置神经网络梯度
    self.policy_optimizer.zero_grad()
    self.value_optimizer.zero_grad()
    
    # 清空经验回放缓冲区
    self.replay_buffer.clear()
    
    # 重置步数计数器
    self.step_count = 0
```

#### 5.7.2 随机初始化
```python
def init_random(self):
    """随机初始化组件位置"""
    # 随机位置
    x = self.rng.uniform(0, self.parameters.board_width - self.node.get_size()[0])
    y = self.rng.uniform(0, self.parameters.board_height - self.node.get_size()[1])
    
    # 随机方向
    orientation = self.rng.uniform(0, 360)
    
    # 设置位置和方向
    self.node.set_pos([x, y])
    self.node.set_orientation(orientation)
```

## 关键数据结构

### Agent类
- **功能**：单个组件的智能体
- **包含**：策略网络、价值网络、经验回放缓冲区

### AgentParameters类
- **功能**：智能体参数配置
- **包含**：环境参数、网络参数、奖励权重

### ReplayBuffer类
- **功能**：经验回放缓冲区
- **包含**：状态、动作、奖励、下一状态的存储

## 输出信息
```
[INFO] Creating agent for component: C3
[INFO] Agent observation space: 24 dimensions
[INFO] Agent action space: 3 dimensions
[INFO] Policy network initialized
[INFO] Value network initialized
[INFO] Replay buffer initialized with size: 10000
```

## 下一步
进入[06_模型设置阶段](./06_模型设置阶段.md) 