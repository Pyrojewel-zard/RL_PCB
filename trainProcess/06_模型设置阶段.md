# 06. 模型设置阶段

## 概述
模型设置阶段负责创建和配置强化学习模型，包括SAC（Soft Actor-Critic）或TD3（Twin Delayed Deep Deterministic Policy Gradient）算法，以及相关的神经网络架构。

## 详细流程

### 6.1 模型类型选择

#### 6.1.1 策略选择
```python
def setup_model(model_type, train_env, hyperparameters, device, early_stopping, verbose):
    """设置强化学习模型"""
    if model_type == "SAC":
        return setup_sac_model(train_env, hyperparameters, device, early_stopping, verbose)
    elif model_type == "TD3":
        return setup_td3_model(train_env, hyperparameters, device, early_stopping, verbose)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
```

#### 6.1.2 SAC模型设置
```python
def setup_sac_model(train_env, hyperparameters, device, early_stopping, verbose):
    """设置SAC模型"""
    # SAC超参数
    sac_params = {
        "learning_rate": hyperparameters.get("learning_rate", 3e-4),
        "buffer_size": hyperparameters.get("buffer_size", 1000000),
        "learning_starts": hyperparameters.get("learning_starts", 100),
        "batch_size": hyperparameters.get("batch_size", 256),
        "tau": hyperparameters.get("tau", 0.005),
        "gamma": hyperparameters.get("gamma", 0.99),
        "train_freq": hyperparameters.get("train_freq", 1),
        "gradient_steps": hyperparameters.get("gradient_steps", 1),
        "ent_coef": hyperparameters.get("ent_coef", "auto"),
        "target_update_interval": hyperparameters.get("target_update_interval", 1),
        "target_entropy": hyperparameters.get("target_entropy", "auto"),
        "use_sde": hyperparameters.get("use_sde", False),
        "sde_sample_freq": hyperparameters.get("sde_sample_freq", -1),
        "use_sde_at_warmup": hyperparameters.get("use_sde_at_warmup", False),
        "policy_kwargs": hyperparameters.get("policy_kwargs", {}),
        "verbose": verbose,
        "device": device,
        "early_stopping": early_stopping
    }
    
    # 创建SAC模型
    model = SAC("MlpPolicy", train_env, **sac_params)
    
    return model
```

#### 6.1.3 TD3模型设置
```python
def setup_td3_model(train_env, hyperparameters, device, early_stopping, verbose):
    """设置TD3模型"""
    # TD3超参数
    td3_params = {
        "learning_rate": hyperparameters.get("learning_rate", 3e-4),
        "buffer_size": hyperparameters.get("buffer_size", 1000000),
        "learning_starts": hyperparameters.get("learning_starts", 100),
        "batch_size": hyperparameters.get("batch_size", 256),
        "tau": hyperparameters.get("tau", 0.005),
        "gamma": hyperparameters.get("gamma", 0.99),
        "train_freq": hyperparameters.get("train_freq", 1),
        "gradient_steps": hyperparameters.get("gradient_steps", 1),
        "policy_delay": hyperparameters.get("policy_delay", 2),
        "target_policy_noise": hyperparameters.get("target_policy_noise", 0.2),
        "target_noise_clip": hyperparameters.get("target_noise_clip", 0.5),
        "policy_kwargs": hyperparameters.get("policy_kwargs", {}),
        "verbose": verbose,
        "device": device,
        "early_stopping": early_stopping
    }
    
    # 创建TD3模型
    model = TD3("MlpPolicy", train_env, **td3_params)
    
    return model
```

### 6.2 神经网络架构

#### 6.2.1 策略网络架构
```python
class PolicyNetwork(nn.Module):
    def __init__(self, observation_dim, action_dim, hidden_dim=256):
        super(PolicyNetwork, self).__init__()
        
        self.fc1 = nn.Linear(observation_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, action_dim)
        
        # 激活函数
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.tanh(self.fc4(x))  # 输出范围[-1, 1]
        
        return x
```

#### 6.2.2 价值网络架构
```python
class ValueNetwork(nn.Module):
    def __init__(self, observation_dim, action_dim, hidden_dim=256):
        super(ValueNetwork, self).__init__()
        
        # Q1网络
        self.q1_fc1 = nn.Linear(observation_dim + action_dim, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc4 = nn.Linear(hidden_dim, 1)
        
        # Q2网络（SAC使用双Q网络）
        self.q2_fc1 = nn.Linear(observation_dim + action_dim, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc4 = nn.Linear(hidden_dim, 1)
        
        self.relu = nn.ReLU()
        
    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        
        # Q1值
        q1 = self.relu(self.q1_fc1(x))
        q1 = self.relu(self.q1_fc2(q1))
        q1 = self.relu(self.q1_fc3(q1))
        q1 = self.q1_fc4(q1)
        
        # Q2值
        q2 = self.relu(self.q2_fc1(x))
        q2 = self.relu(self.q2_fc2(q2))
        q2 = self.relu(self.q2_fc3(q2))
        q2 = self.q2_fc4(q2)
        
        return q1, q2
```

### 6.3 超参数配置

#### 6.3.1 SAC超参数文件
```json
{
    "learning_rate": 3e-4,
    "buffer_size": 1000000,
    "learning_starts": 1000,
    "batch_size": 256,
    "tau": 0.005,
    "gamma": 0.99,
    "train_freq": 1,
    "gradient_steps": 1,
    "ent_coef": "auto",
    "target_update_interval": 1,
    "target_entropy": "auto",
    "use_sde": false,
    "sde_sample_freq": -1,
    "use_sde_at_warmup": false,
    "policy_kwargs": {
        "net_arch": [256, 256],
        "activation_fn": "relu"
    }
}
```

#### 6.3.2 TD3超参数文件
```json
{
    "learning_rate": 3e-4,
    "buffer_size": 1000000,
    "learning_starts": 1000,
    "batch_size": 256,
    "tau": 0.005,
    "gamma": 0.99,
    "train_freq": 1,
    "gradient_steps": 1,
    "policy_delay": 2,
    "target_policy_noise": 0.2,
    "target_noise_clip": 0.5,
    "policy_kwargs": {
        "net_arch": [256, 256],
        "activation_fn": "relu"
    }
}
```

### 6.4 设备配置

#### 6.4.1 GPU/CPU设备选择
```python
def setup_device(device_str):
    """设置计算设备"""
    if device_str == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device("cpu")
        print("Using CPU")
    
    return device
```

#### 6.4.2 模型设备迁移
```python
def move_model_to_device(model, device):
    """将模型移动到指定设备"""
    model.policy = model.policy.to(device)
    model.q_net = model.q_net.to(device)
    model.q_net_target = model.q_net_target.to(device)
    
    return model
```

### 6.5 经验回放缓冲区

#### 6.5.1 缓冲区初始化
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
        
    def push(self, state, action, reward, next_state, done):
        """添加经验到缓冲区"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
        
    def sample(self, batch_size):
        """采样经验批次"""
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        
        return state, action, reward, next_state, done
        
    def __len__(self):
        return len(self.buffer)
```

### 6.6 目标网络更新

#### 6.6.1 软更新（SAC）
```python
def soft_update(target, source, tau):
    """软更新目标网络"""
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - tau) + param.data * tau
        )
```

#### 6.6.2 硬更新（TD3）
```python
def hard_update(target, source):
    """硬更新目标网络"""
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)
```

### 6.7 探索策略

#### 6.7.1 高斯噪声探索
```python
def add_gaussian_noise(action, noise_std=0.1):
    """添加高斯噪声到动作"""
    noise = np.random.normal(0, noise_std, action.shape)
    noisy_action = action + noise
    return np.clip(noisy_action, -1, 1)
```

#### 6.7.2 Ornstein-Uhlenbeck噪声
```python
class OUNoise:
    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.reset()
        
    def reset(self):
        self.state = np.ones(self.action_dim) * self.mu
        
    def sample(self):
        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)
        self.state += dx
        return self.state
```

## 关键数据结构

### SAC类
- **功能**：Soft Actor-Critic算法实现
- **包含**：策略网络、价值网络、目标网络

### TD3类
- **功能**：Twin Delayed Deep Deterministic Policy Gradient算法实现
- **包含**：策略网络、双Q网络、目标网络

### ReplayBuffer类
- **功能**：经验回放缓冲区
- **包含**：状态、动作、奖励、下一状态的存储

## 输出信息
```
[INFO] Setting up SAC model
[INFO] Learning rate: 0.0003
[INFO] Buffer size: 1000000
[INFO] Batch size: 256
[INFO] Device: cuda
[INFO] Policy network architecture: [256, 256]
[INFO] Value network architecture: [256, 256]
[INFO] Target entropy: auto
[INFO] Model setup completed
```

## 下一步
进入[07_回调函数配置阶段](./07_回调函数配置阶段.md) 