 # 07. 回调函数配置阶段

## 概述
回调函数配置阶段负责设置训练过程中的监控、评估、日志记录和模型保存功能，确保训练过程的可视化和可追踪性。

## 详细流程

### 7.1 回调函数初始化

#### 7.1.1 回调函数构造函数
```python
class log_and_eval_callback():
    def __init__(
            self,
            log_dir: str,
            settings: dict,
            hyperparameters: dict,
            model,
            verbose: int = 1,
            num_evaluations: int = 10,
            eval_freq: int = 10_000,
            training_log: str = None,
            pcb_save_freq: int = None,  # PCB保存频率参数
        ):
        
        super().__init__()
        self.log_dir = log_dir
        # 根日志目录
        self.save_path = self.log_dir
        # 保存模型的子目录
        self.model_path = os.path.join(self.save_path, "models")
        # 评估视频的子目录
        self.video_path = os.path.join(self.save_path, "video_dir")
        # 基于训练PCB文件的评估子目录
        self.video_train_path = os.path.join(self.video_path, "training_set")
        # 基于评估PCB文件的评估子目录
        self.video_eval_path = os.path.join(self.video_path, "evaluation_set")
        # 实时PCB保存目录
        self.realtime_pcb_path = os.path.join(self.save_path, "realtime_pcb")
        # 保存最佳线长的文件
        self.optimals = os.path.join(self.save_path, "wirelength.optimals")
        
        self.verbose = verbose
        self.num_evaluations = num_evaluations
        self.eval_freq = eval_freq
        # PCB保存频率，默认为None表示不保存
        self.pcb_save_freq = pcb_save_freq
        
        # 训练日志文件
        if training_log is not None:
            self.training_log = os.path.join(self.save_path, training_log)
        else:
            self.training_log = training_log
```

### 7.2 目录结构创建

#### 7.2.1 目录初始化
```python
def _init_directories(self):
    """初始化必要的目录"""
    # 创建模型保存目录
    if not os.path.exists(self.model_path):
        os.makedirs(self.model_path)
    
    # 创建视频保存目录
    if not os.path.exists(self.video_path):
        os.makedirs(self.video_path)
    if not os.path.exists(self.video_train_path):
        os.makedirs(self.video_train_path)
    if not os.path.exists(self.video_eval_path):
        os.makedirs(self.video_eval_path)
    
    # 创建实时PCB保存目录
    if self.pcb_save_freq is not None:
        if not os.path.exists(self.realtime_pcb_path):
            os.makedirs(self.realtime_pcb_path)
```

### 7.3 TensorBoard日志记录

#### 7.3.1 TensorBoard写入器初始化
```python
def _init_tensorboard(self):
    """初始化TensorBoard写入器"""
    self.writer = SummaryWriter(log_dir=self.log_dir)
    
    # 记录训练配置
    self.writer.add_text(
        "Training Configuration",
        self._format_settings_text(self.settings),
        global_step=0
    )
    
    # 记录超参数
    self.writer.add_text(
        "Hyperparameters",
        self._format_hyperparameters_text(self.hyperparameters),
        global_step=0
    )
```

#### 7.3.2 训练指标记录
```python
def _log_training_metrics(self, global_step, metrics):
    """记录训练指标"""
    # 记录奖励
    if 'reward' in metrics:
        self.writer.add_scalar('Training/Reward', metrics['reward'], global_step)
    
    # 记录损失
    if 'policy_loss' in metrics:
        self.writer.add_scalar('Training/Policy_Loss', metrics['policy_loss'], global_step)
    if 'value_loss' in metrics:
        self.writer.add_scalar('Training/Value_Loss', metrics['value_loss'], global_step)
    
    # 记录探索噪声
    if 'exploration_noise' in metrics:
        self.writer.add_scalar('Training/Exploration_Noise', metrics['exploration_noise'], global_step)
    
    # 记录学习率
    if 'learning_rate' in metrics:
        self.writer.add_scalar('Training/Learning_Rate', metrics['learning_rate'], global_step)
```

### 7.4 模型评估

#### 7.4.1 评估函数
```python
def _evaluate_model(self, model, env, num_episodes=10):
    """评估模型性能"""
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward
            episode_length += 1
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
    
    return {
        'mean_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'mean_length': np.mean(episode_lengths),
        'std_length': np.std(episode_lengths)
    }
```

#### 7.4.2 评估回调
```python
def on_evaluation(self, global_step):
    """评估回调函数"""
    if global_step % self.eval_freq == 0:
        # 创建评估环境
        eval_env = self._create_evaluation_env()
        
        # 执行评估
        eval_results = self._evaluate_model(self.model, eval_env, self.num_evaluations)
        
        # 记录评估结果
        self.writer.add_scalar('Evaluation/Mean_Reward', eval_results['mean_reward'], global_step)
        self.writer.add_scalar('Evaluation/Std_Reward', eval_results['std_reward'], global_step)
        self.writer.add_scalar('Evaluation/Mean_Length', eval_results['mean_length'], global_step)
        
        # 保存最佳模型
        if eval_results['mean_reward'] > self.best_reward:
            self.best_reward = eval_results['mean_reward']
            self._save_best_model(global_step)
        
        # 生成评估视频
        self._generate_evaluation_video(eval_env, global_step)
```

### 7.5 模型保存

#### 7.5.1 最佳模型保存
```python
def _save_best_model(self, global_step):
    """保存最佳模型"""
    model_path = os.path.join(self.model_path, f"best_model_{global_step}.pth")
    
    # 保存模型状态
    torch.save({
        'model_state_dict': self.model.policy.state_dict(),
        'optimizer_state_dict': self.model.policy_optimizer.state_dict(),
        'global_step': global_step,
        'best_reward': self.best_reward,
        'settings': self.settings,
        'hyperparameters': self.hyperparameters
    }, model_path)
    
    print(f"Best model saved at step {global_step} with reward {self.best_reward:.2f}")
```

#### 7.5.2 定期模型保存
```python
def _save_checkpoint(self, global_step):
    """保存检查点"""
    if global_step % 50000 == 0:  # 每5万步保存一次
        checkpoint_path = os.path.join(self.model_path, f"checkpoint_{global_step}.pth")
        
        torch.save({
            'model_state_dict': self.model.policy.state_dict(),
            'optimizer_state_dict': self.model.policy_optimizer.state_dict(),
            'global_step': global_step,
            'settings': self.settings,
            'hyperparameters': self.hyperparameters
        }, checkpoint_path)
        
        print(f"Checkpoint saved at step {global_step}")
```

### 7.6 PCB文件保存

#### 7.6.1 实时PCB保存
```python
def _save_realtime_pcb(self, global_step):
    """保存实时PCB文件"""
    if self.pcb_save_freq is not None and global_step % self.pcb_save_freq == 0:
        # 获取当前PCB状态
        current_pcb = self._get_current_pcb_state()
        
        # 保存PCB文件
        pcb_filename = f"realtime_pcb_step_{global_step}.pcb"
        pcb_path = os.path.join(self.realtime_pcb_path, pcb_filename)
        
        self._write_pcb_file(current_pcb, pcb_path)
        
        print(f"Real-time PCB saved at step {global_step}: {pcb_path}")
```

#### 7.6.2 PCB状态获取
```python
def _get_current_pcb_state(self):
    """获取当前PCB状态"""
    # 获取所有组件的位置和方向
    components = []
    for agent in self.env.agents:
        node = agent.node
        pos = node.get_pos()
        orientation = node.get_orientation()
        size = node.get_size()
        
        components.append({
            'id': node.get_id(),
            'name': node.get_name(),
            'x': pos[0],
            'y': pos[1],
            'width': size[0],
            'height': size[1],
            'orientation': orientation
        })
    
    return {
        'components': components,
        'board_width': self.env.b.get_width(),
        'board_height': self.env.b.get_height(),
        'timestamp': time.time()
    }
```

### 7.7 训练日志记录

#### 7.7.1 训练日志写入
```python
def _write_training_log(self, global_step, metrics):
    """写入训练日志"""
    if self.training_log is not None:
        log_entry = {
            'step': global_step,
            'timestamp': time.time(),
            'reward': metrics.get('reward', 0),
            'policy_loss': metrics.get('policy_loss', 0),
            'value_loss': metrics.get('value_loss', 0),
            'exploration_noise': metrics.get('exploration_noise', 0),
            'learning_rate': metrics.get('learning_rate', 0)
        }
        
        with open(self.training_log, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
```

### 7.8 视频生成

#### 7.8.1 评估视频生成
```python
def _generate_evaluation_video(self, env, global_step):
    """生成评估视频"""
    # 创建视频保存路径
    video_filename = f"evaluation_step_{global_step}.mp4"
    video_path = os.path.join(self.video_eval_path, video_filename)
    
    # 录制评估过程
    frames = []
    obs = env.reset()
    
    while True:
        # 渲染当前帧
        frame = env.render(mode='rgb_array')
        frames.append(frame)
        
        # 执行动作
        action, _ = self.model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        
        if done:
            break
    
    # 保存视频
    self._save_video(frames, video_path)
    print(f"Evaluation video saved: {video_path}")
```

### 7.9 回调函数注册

#### 7.9.1 回调函数注册
```python
def register_callbacks(self, model):
    """注册回调函数到模型"""
    # 设置回调函数
    model.set_callback(self)
    
    # 初始化目录
    self._init_directories()
    
    # 初始化TensorBoard
    self._init_tensorboard()
    
    # 初始化最佳奖励
    self.best_reward = float('-inf')
    
    print(f"Callbacks registered. Log directory: {self.log_dir}")
```

## 关键数据结构

### log_and_eval_callback类
- **功能**：训练监控和评估回调
- **包含**：TensorBoard记录、模型保存、评估执行

### SummaryWriter类
- **功能**：TensorBoard日志写入器
- **包含**：标量、文本、图像等数据记录

### ReplayBuffer类
- **功能**：经验回放缓冲区
- **包含**：状态、动作、奖励、下一状态的存储

## 输出信息
```
[INFO] Callbacks registered. Log directory: /path/to/work/experiment_xxx
[INFO] TensorBoard writer initialized
[INFO] Model evaluation frequency: 10000 steps
[INFO] PCB save frequency: 1000 steps
[INFO] Video generation enabled
[INFO] Best model tracking enabled
```

## 下一步
进入[08_探索阶段](./08_探索阶段.md)