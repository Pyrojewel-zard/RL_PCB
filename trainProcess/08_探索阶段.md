 # 08. 探索阶段

## 概述
探索阶段是强化学习训练的重要环节，智能体通过随机探索来收集初始经验，为后续的策略学习提供数据基础。在PCB布局优化中，探索阶段帮助智能体了解组件放置的可行空间和奖励结构。

## 详细流程

### 8.1 探索阶段启动

#### 8.1.1 探索函数调用
```python
def explore_for_expert_targets(self, reward_target_exploration_steps, output_dir, save_pcb_every_n_steps):
    """为目标奖励进行探索"""
    print(f"Starting exploration phase for {reward_target_exploration_steps} steps...")
    
    # 初始化探索统计
    exploration_stats = {
        'total_steps': 0,
        'episodes': 0,
        'total_reward': 0,
        'best_reward': float('-inf'),
        'exploration_noise': 1.0,
        'pcb_saves': 0
    }
    
    # 开始探索循环
    for step in range(reward_target_exploration_steps):
        # 执行探索步骤
        exploration_result = self._exploration_step(exploration_stats)
        
        # 更新统计信息
        exploration_stats['total_steps'] += 1
        exploration_stats['total_reward'] += exploration_result['reward']
        
        # 更新最佳奖励
        if exploration_result['reward'] > exploration_stats['best_reward']:
            exploration_stats['best_reward'] = exploration_result['reward']
        
        # 保存PCB文件（如果启用）
        if save_pcb_every_n_steps > 0 and step % save_pcb_every_n_steps == 0:
            self._save_exploration_pcb(output_dir, step, exploration_stats)
            exploration_stats['pcb_saves'] += 1
        
        # 定期输出探索进度
        if step % 1000 == 0:
            self._log_exploration_progress(step, exploration_stats)
    
    print(f"Exploration phase completed. Best reward: {exploration_stats['best_reward']:.2f}")
    return exploration_stats
```

### 8.2 探索步骤执行

#### 8.2.1 单步探索
```python
def _exploration_step(self, exploration_stats):
    """执行单步探索"""
    # 获取当前观察
    observation = self._get_current_observation()
    
    # 生成随机动作（高探索噪声）
    action = self._generate_exploration_action(exploration_stats['exploration_noise'])
    
    # 执行动作
    next_observation, reward, done, info = self._execute_action(action)
    
    # 存储经验到回放缓冲区
    self.replay_buffer.push(
        state=observation,
        action=action,
        reward=reward,
        next_state=next_observation,
        done=done
    )
    
    # 重置环境（如果episode结束）
    if done:
        self._reset_environment()
        exploration_stats['episodes'] += 1
    
    return {
        'reward': reward,
        'action': action,
        'done': done,
        'info': info
    }
```

#### 8.2.2 探索动作生成
```python
def _generate_exploration_action(self, exploration_noise):
    """生成探索动作"""
    # 基础随机动作
    base_action = np.random.uniform(-1, 1, size=self.action_dim)
    
    # 添加高斯噪声
    noise = np.random.normal(0, exploration_noise, size=self.action_dim)
    noisy_action = base_action + noise
    
    # 裁剪到有效范围
    exploration_action = np.clip(noisy_action, -1, 1)
    
    return exploration_action
```

### 8.3 观察空间构建

#### 8.3.1 当前观察获取
```python
def _get_current_observation(self):
    """获取当前观察"""
    obs = []
    
    # 1. 组件自身信息
    component_info = self._get_component_info()
    obs.extend(component_info)
    
    # 2. 邻居组件信息
    neighbor_info = self._get_neighbor_info()
    obs.extend(neighbor_info)
    
    # 3. 连接信息
    connection_info = self._get_connection_info()
    obs.extend(connection_info)
    
    # 4. 电路板边界信息
    board_info = self._get_board_info()
    obs.extend(board_info)
    
    return np.array(obs, dtype=np.float32)
```

#### 8.3.2 组件信息获取
```python
def _get_component_info(self):
    """获取组件自身信息"""
    pos = self.node.get_pos()
    size = self.node.get_size()
    orientation = self.node.get_orientation()
    
    return [
        pos[0],      # X坐标
        pos[1],      # Y坐标
        size[0],     # 宽度
        size[1]      # 高度
    ]
```

#### 8.3.3 邻居信息获取
```python
def _get_neighbor_info(self):
    """获取邻居组件信息"""
    neighbor_info = []
    
    for neighbor in self.neighbors:
        n_pos = neighbor.get_pos()
        n_size = neighbor.get_size()
        
        neighbor_info.extend([
            n_pos[0],    # 邻居X坐标
            n_pos[1],    # 邻居Y坐标
            n_size[0],   # 邻居宽度
            n_size[1]    # 邻居高度
        ])
    
    return neighbor_info
```

#### 8.3.4 连接信息获取
```python
def _get_connection_info(self):
    """获取连接信息"""
    connection_info = []
    
    for edge in self.eoi:
        connection_info.extend([
            edge.get_net_id(),      # 网络ID
            edge.get_power_rail()   # 电源轨道
        ])
    
    return connection_info
```

#### 8.3.5 电路板信息获取
```python
def _get_board_info(self):
    """获取电路板边界信息"""
    return [
        self.parameters.board_width,   # 电路板宽度
        self.parameters.board_height,  # 电路板高度
        0,                             # 最小X边界
        0                              # 最小Y边界
    ]
```

### 8.4 动作执行

#### 8.4.1 动作执行函数
```python
def _execute_action(self, action):
    """执行动作并返回结果"""
    # 解析动作
    translation_x = action[0] * self.action_space['translation_x'][1]
    translation_y = action[1] * self.action_space['translation_y'][1]
    rotation = action[2] * self.action_space['rotation'][1]
    
    # 获取当前位置
    current_pos = self.node.get_pos()
    current_orientation = self.node.get_orientation()
    
    # 计算新位置
    new_x = current_pos[0] + translation_x
    new_y = current_pos[1] + translation_y
    new_orientation = (current_orientation + rotation) % 360
    
    # 边界检查
    new_x = np.clip(new_x, 0, self.parameters.board_width - self.node.get_size()[0])
    new_y = np.clip(new_y, 0, self.parameters.board_height - self.node.get_size()[1])
    
    # 更新组件位置
    self.node.set_pos([new_x, new_y])
    self.node.set_orientation(new_orientation)
    
    # 计算奖励
    reward = self._calculate_reward()
    
    # 检查是否完成
    done = self._check_episode_done()
    
    # 获取下一观察
    next_observation = self._get_current_observation()
    
    return next_observation, reward, done, {}
```

### 8.5 奖励计算

#### 8.5.1 探索阶段奖励
```python
def _calculate_exploration_reward(self):
    """计算探索阶段的奖励"""
    reward = 0.0
    
    # 1. 欧几里得距离奖励
    euclidean_distance = self._calculate_euclidean_distance()
    reward += self.parameters.n * euclidean_distance
    
    # 2. HPWL奖励
    hpwl = self._calculate_hpwl()
    reward += self.parameters.p * hpwl
    
    # 3. 重叠惩罚
    overlap_penalty = self._calculate_overlap_penalty()
    reward += self.parameters.m * overlap_penalty
    
    # 4. 探索奖励（鼓励探索新区域）
    exploration_bonus = self._calculate_exploration_bonus()
    reward += exploration_bonus
    
    return reward
```

#### 8.5.2 探索奖励计算
```python
def _calculate_exploration_bonus(self):
    """计算探索奖励"""
    # 检查是否访问了新区域
    current_pos = self.node.get_pos()
    
    # 计算到已访问区域的距离
    min_distance_to_visited = float('inf')
    for visited_pos in self.visited_positions:
        distance = np.sqrt(
            (current_pos[0] - visited_pos[0])**2 + 
            (current_pos[1] - visited_pos[1])**2
        )
        min_distance_to_visited = min(min_distance_to_visited, distance)
    
    # 如果距离足够远，给予探索奖励
    if min_distance_to_visited > self.exploration_threshold:
        exploration_bonus = 0.1
        self.visited_positions.append(current_pos)
    else:
        exploration_bonus = 0.0
    
    return exploration_bonus
```

### 8.6 PCB文件保存

#### 8.6.1 探索PCB保存
```python
def _save_exploration_pcb(self, output_dir, step, exploration_stats):
    """保存探索阶段的PCB文件"""
    # 创建PCB保存目录
    pcb_dir = os.path.join(output_dir, "exploration_pcb")
    if not os.path.exists(pcb_dir):
        os.makedirs(pcb_dir)
    
    # 生成PCB文件名
    pcb_filename = f"exploration_step_{step}_reward_{exploration_stats['best_reward']:.2f}.pcb"
    pcb_path = os.path.join(pcb_dir, pcb_filename)
    
    # 获取当前PCB状态
    current_pcb_state = self._get_current_pcb_state()
    
    # 写入PCB文件
    self._write_pcb_file(current_pcb_state, pcb_path)
    
    print(f"Exploration PCB saved: {pcb_path}")
```

#### 8.6.2 PCB状态获取
```python
def _get_current_pcb_state(self):
    """获取当前PCB状态"""
    # 获取所有组件信息
    components = []
    
    # 当前组件
    current_component = {
        'id': self.node.get_id(),
        'name': self.node.get_name(),
        'x': self.node.get_pos()[0],
        'y': self.node.get_pos()[1],
        'width': self.node.get_size()[0],
        'height': self.node.get_size()[1],
        'orientation': self.node.get_orientation(),
        'is_placed': self.node.get_isPlaced()
    }
    components.append(current_component)
    
    # 邻居组件
    for neighbor in self.neighbors:
        neighbor_component = {
            'id': neighbor.get_id(),
            'name': neighbor.get_name(),
            'x': neighbor.get_pos()[0],
            'y': neighbor.get_pos()[1],
            'width': neighbor.get_size()[0],
            'height': neighbor.get_size()[1],
            'orientation': neighbor.get_orientation(),
            'is_placed': neighbor.get_isPlaced()
        }
        components.append(neighbor_component)
    
    return {
        'components': components,
        'board_width': self.parameters.board_width,
        'board_height': self.parameters.board_height,
        'timestamp': time.time()
    }
```

### 8.7 探索进度记录

#### 8.7.1 进度日志
```python
def _log_exploration_progress(self, step, exploration_stats):
    """记录探索进度"""
    avg_reward = exploration_stats['total_reward'] / (step + 1)
    episodes = exploration_stats['episodes']
    
    print(f"Exploration Step {step}: "
          f"Episodes={episodes}, "
          f"Avg Reward={avg_reward:.3f}, "
          f"Best Reward={exploration_stats['best_reward']:.3f}, "
          f"PCB Saves={exploration_stats['pcb_saves']}")
    
    # 记录到TensorBoard
    if hasattr(self, 'writer'):
        self.writer.add_scalar('Exploration/Average_Reward', avg_reward, step)
        self.writer.add_scalar('Exploration/Best_Reward', exploration_stats['best_reward'], step)
        self.writer.add_scalar('Exploration/Episodes', episodes, step)
```

### 8.8 环境重置

#### 8.8.1 环境重置函数
```python
def _reset_environment(self):
    """重置环境状态"""
    # 重置所有智能体
    for agent in self.env.agents:
        agent.reset()
    
    # 重置环境状态
    self.env.reset()
    
    # 清空访问位置记录
    self.visited_positions = []
    
    # 重置探索阈值
    self.exploration_threshold = 5.0  # 5mm的探索阈值
```

## 关键数据结构

### ExplorationStats类
- **功能**：探索统计信息
- **包含**：步数、episode数、奖励、最佳奖励等

### ReplayBuffer类
- **功能**：经验回放缓冲区
- **包含**：探索阶段收集的经验数据

### VisitedPositions类
- **功能**：已访问位置记录
- **包含**：探索过程中访问过的位置坐标

## 输出信息
```
[INFO] Starting exploration phase for 10000 steps...
[INFO] Exploration Step 1000: Episodes=5, Avg Reward=-2.345, Best Reward=-1.234, PCB Saves=1
[INFO] Exploration Step 2000: Episodes=12, Avg Reward=-1.987, Best Reward=-0.876, PCB Saves=2
[INFO] Exploration phase completed. Best reward: -0.876
```

## 下一步
进入[09_训练阶段](./09_训练阶段.md)