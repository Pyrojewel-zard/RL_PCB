# 09. 训练阶段

## 概述
训练阶段是强化学习的核心环节，智能体通过经验回放和学习算法来优化策略网络和价值网络，逐步提高PCB组件布局的质量。

## 详细流程

### 9.1 训练阶段启动

#### 9.1.1 训练函数调用
```python
def learn(self, timesteps, callback, start_timesteps, incremental_replay_buffer):
    """开始训练"""
    print(f"Starting training phase for {timesteps} timesteps...")
    
    # 初始化训练统计
    training_stats = {
        'total_timesteps': 0,
        'episodes': 0,
        'policy_loss': 0,
        'value_loss': 0,
        'reward': 0,
        'learning_rate': self.learning_rate,
        'exploration_noise': self.exploration_noise
    }
    
    # 训练循环
    for timestep in range(timesteps):
        # 执行训练步骤
        training_result = self._training_step(training_stats)
        
        # 更新统计信息
        training_stats['total_timesteps'] += 1
        training_stats['reward'] += training_result['reward']
        
        # 更新损失
        if 'policy_loss' in training_result:
            training_stats['policy_loss'] = training_result['policy_loss']
        if 'value_loss' in training_result:
            training_stats['value_loss'] = training_result['value_loss']
        
        # 回调函数处理
        if callback is not None:
            callback.on_step(timestep, training_result)
        
        # 定期输出训练进度
        if timestep % 1000 == 0:
            self._log_training_progress(timestep, training_stats)
    
    print(f"Training phase completed. Final average reward: {training_stats['reward']/timesteps:.3f}")
    return training_stats
```

### 9.2 训练步骤执行

#### 9.2.1 单步训练
```python
def _training_step(self, training_stats):
    """执行单步训练"""
    # 获取当前观察
    observation = self._get_current_observation()
    
    # 选择动作（探索或利用）
    if self._should_explore(training_stats['total_timesteps']):
        action = self._generate_exploration_action(training_stats['exploration_noise'])
    else:
        action = self._generate_exploitation_action(observation)
    
    # 执行动作
    next_observation, reward, done, info = self._execute_action(action)
    
    # 存储经验
    self.replay_buffer.push(
        state=observation,
        action=action,
        reward=reward,
        next_state=next_observation,
        done=done
    )
    
    # 学习（如果缓冲区足够大）
    if len(self.replay_buffer) >= self.batch_size:
        learning_result = self._learning_step()
        training_stats.update(learning_result)
    
    # 重置环境（如果episode结束）
    if done:
        self._reset_environment()
        training_stats['episodes'] += 1
    
    return {
        'reward': reward,
        'action': action,
        'done': done,
        'policy_loss': training_stats.get('policy_loss', 0),
        'value_loss': training_stats.get('value_loss', 0)
    }
```

### 9.3 动作选择策略

#### 9.3.1 探索vs利用决策
```python
def _should_explore(self, timestep):
    """决定是否进行探索"""
    # 在训练初期，更多探索
    if timestep < self.start_timesteps:
        return True
    
    # 使用epsilon-greedy策略
    epsilon = self._calculate_epsilon(timestep)
    return np.random.random() < epsilon
```

#### 9.3.2 Epsilon计算
```python
def _calculate_epsilon(self, timestep):
    """计算探索概率epsilon"""
    # 从1.0线性衰减到0.01
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 100000  # 衰减步数
    
    epsilon = epsilon_start - (epsilon_start - epsilon_end) * (timestep / epsilon_decay)
    return max(epsilon, epsilon_end)
```

#### 9.3.3 利用动作生成
```python
def _generate_exploitation_action(self, observation):
    """生成利用动作（基于策略网络）"""
    # 将观察转换为tensor
    obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)
    
    # 通过策略网络获取动作
    with torch.no_grad():
        action = self.policy_net(obs_tensor)
        action = action.cpu().numpy()[0]
    
    # 添加少量噪声（用于持续探索）
    noise = np.random.normal(0, 0.01, action.shape)
    action = action + noise
    
    return np.clip(action, -1, 1)
```

### 9.4 学习步骤

#### 9.4.1 SAC学习算法
```python
def _learning_step(self):
    """执行学习步骤（SAC算法）"""
    # 采样经验批次
    batch = self.replay_buffer.sample(self.batch_size)
    states, actions, rewards, next_states, dones = batch
    
    # 转换为tensor
    states = torch.FloatTensor(states).to(self.device)
    actions = torch.FloatTensor(actions).to(self.device)
    rewards = torch.FloatTensor(rewards).to(self.device)
    next_states = torch.FloatTensor(next_states).to(self.device)
    dones = torch.FloatTensor(dones).to(self.device)
    
    # 计算目标Q值
    with torch.no_grad():
        next_actions, next_log_probs = self.policy_net.sample(next_states)
        next_q1, next_q2 = self.q_net_target(next_states, next_actions)
        next_q = torch.min(next_q1, next_q2)
        target_q = rewards + (1 - dones) * self.gamma * (next_q - self.alpha * next_log_probs)
    
    # 更新Q网络
    current_q1, current_q2 = self.q_net(states, actions)
    q1_loss = F.mse_loss(current_q1, target_q)
    q2_loss = F.mse_loss(current_q2, target_q)
    q_loss = q1_loss + q2_loss
    
    self.q_optimizer.zero_grad()
    q_loss.backward()
    self.q_optimizer.step()
    
    # 更新策略网络
    new_actions, log_probs = self.policy_net.sample(states)
    q1_new, q2_new = self.q_net(states, new_actions)
    q_new = torch.min(q1_new, q2_new)
    policy_loss = (self.alpha * log_probs - q_new).mean()
    
    self.policy_optimizer.zero_grad()
    policy_loss.backward()
    self.policy_optimizer.step()
    
    # 软更新目标网络
    self._soft_update_target_networks()
    
    return {
        'policy_loss': policy_loss.item(),
        'value_loss': q_loss.item()
    }
```

#### 9.4.2 TD3学习算法
```python
def _learning_step_td3(self):
    """执行学习步骤（TD3算法）"""
    # 采样经验批次
    batch = self.replay_buffer.sample(self.batch_size)
    states, actions, rewards, next_states, dones = batch
    
    # 转换为tensor
    states = torch.FloatTensor(states).to(self.device)
    actions = torch.FloatTensor(actions).to(self.device)
    rewards = torch.FloatTensor(rewards).to(self.device)
    next_states = torch.FloatTensor(next_states).to(self.device)
    dones = torch.FloatTensor(dones).to(self.device)
    
    # 计算目标Q值
    with torch.no_grad():
        next_actions = self.policy_target(next_states)
        noise = torch.randn_like(next_actions) * self.target_policy_noise
        noise = torch.clamp(noise, -self.target_noise_clip, self.target_noise_clip)
        next_actions = torch.clamp(next_actions + noise, -1, 1)
        
        next_q1, next_q2 = self.q_net_target(next_states, next_actions)
        next_q = torch.min(next_q1, next_q2)
        target_q = rewards + (1 - dones) * self.gamma * next_q
    
    # 更新Q网络
    current_q1, current_q2 = self.q_net(states, actions)
    q1_loss = F.mse_loss(current_q1, target_q)
    q2_loss = F.mse_loss(current_q2, target_q)
    q_loss = q1_loss + q2_loss
    
    self.q_optimizer.zero_grad()
    q_loss.backward()
    self.q_optimizer.step()
    
    # 延迟策略更新
    if self.total_timesteps % self.policy_delay == 0:
        # 更新策略网络
        new_actions = self.policy(states)
        q1_new, q2_new = self.q_net(states, new_actions)
        q_new = torch.min(q1_new, q2_new)
        policy_loss = -q_new.mean()
        
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()
        
        # 硬更新目标网络
        self._hard_update_target_networks()
    
    return {
        'policy_loss': policy_loss.item() if self.total_timesteps % self.policy_delay == 0 else 0,
        'value_loss': q_loss.item()
    }
```

### 9.5 网络更新

#### 9.5.1 软更新（SAC）
```python
def _soft_update_target_networks(self):
    """软更新目标网络"""
    # 更新Q网络目标
    for target_param, param in zip(self.q_net_target.parameters(), self.q_net.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - self.tau) + param.data * self.tau
        )
    
    # 更新策略网络目标
    for target_param, param in zip(self.policy_target.parameters(), self.policy.parameters()):
        target_param.data.copy_(
            target_param.data * (1.0 - self.tau) + param.data * self.tau
        )
```

#### 9.5.2 硬更新（TD3）
```python
def _hard_update_target_networks(self):
    """硬更新目标网络"""
    # 更新Q网络目标
    for target_param, param in zip(self.q_net_target.parameters(), self.q_net.parameters()):
        target_param.data.copy_(param.data)
    
    # 更新策略网络目标
    for target_param, param in zip(self.policy_target.parameters(), self.policy.parameters()):
        target_param.data.copy_(param.data)
```

### 9.6 训练进度记录

#### 9.6.1 进度日志
```python
def _log_training_progress(self, timestep, training_stats):
    """记录训练进度"""
    avg_reward = training_stats['reward'] / (timestep + 1)
    episodes = training_stats['episodes']
    policy_loss = training_stats['policy_loss']
    value_loss = training_stats['value_loss']
    
    print(f"Training Step {timestep}: "
          f"Episodes={episodes}, "
          f"Avg Reward={avg_reward:.3f}, "
          f"Policy Loss={policy_loss:.4f}, "
          f"Value Loss={value_loss:.4f}")
    
    # 记录到TensorBoard
    if hasattr(self, 'writer'):
        self.writer.add_scalar('Training/Average_Reward', avg_reward, timestep)
        self.writer.add_scalar('Training/Policy_Loss', policy_loss, timestep)
        self.writer.add_scalar('Training/Value_Loss', value_loss, timestep)
        self.writer.add_scalar('Training/Episodes', episodes, timestep)
```

### 9.7 模型保存

#### 9.7.1 定期保存
```python
def _save_training_checkpoint(self, timestep, training_stats):
    """保存训练检查点"""
    if timestep % 50000 == 0:  # 每5万步保存一次
        checkpoint = {
            'timestep': timestep,
            'policy_state_dict': self.policy.state_dict(),
            'q_net_state_dict': self.q_net.state_dict(),
            'policy_target_state_dict': self.policy_target.state_dict(),
            'q_net_target_state_dict': self.q_net_target.state_dict(),
            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),
            'q_optimizer_state_dict': self.q_optimizer.state_dict(),
            'training_stats': training_stats,
            'hyperparameters': self.hyperparameters
        }
        
        checkpoint_path = os.path.join(self.model_path, f"checkpoint_{timestep}.pth")
        torch.save(checkpoint, checkpoint_path)
        
        print(f"Training checkpoint saved: {checkpoint_path}")
```

### 9.8 性能监控

#### 9.8.1 性能指标计算
```python
def _calculate_performance_metrics(self, training_stats):
    """计算性能指标"""
    metrics = {
        'average_reward': training_stats['reward'] / training_stats['total_timesteps'],
        'episode_count': training_stats['episodes'],
        'policy_loss': training_stats['policy_loss'],
        'value_loss': training_stats['value_loss'],
        'learning_rate': training_stats['learning_rate'],
        'exploration_noise': training_stats['exploration_noise']
    }
    
    return metrics
```

## 关键数据结构

### TrainingStats类
- **功能**：训练统计信息
- **包含**：步数、episode数、损失、奖励等

### ReplayBuffer类
- **功能**：经验回放缓冲区
- **包含**：训练过程中收集的经验数据

### PolicyNetwork类
- **功能**：策略网络
- **包含**：动作生成和策略优化

### ValueNetwork类
- **功能**：价值网络
- **包含**：Q值估计和价值函数学习

## 输出信息
```
[INFO] Starting training phase for 600000 timesteps...
[INFO] Training Step 1000: Episodes=15, Avg Reward=-1.234, Policy Loss=0.0456, Value Loss=0.0234
[INFO] Training Step 2000: Episodes=32, Avg Reward=-0.987, Policy Loss=0.0321, Value Loss=0.0187
[INFO] Training checkpoint saved: /path/to/models/checkpoint_50000.pth
[INFO] Training phase completed. Final average reward: -0.456
```

## 下一步
进入[10_评估阶段](./10_评估阶段.md) 